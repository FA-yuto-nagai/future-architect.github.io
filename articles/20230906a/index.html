<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <!--
    ███████╗██╗░░░██╗████████╗██╗░░░██╗██████╗░███████╗
    ██╔════╝██║░░░██║╚══██╔══╝██║░░░██║██╔══██╗██╔════╝
    █████╗░░██║░░░██║░░░██║░░░██║░░░██║██████╔╝█████╗░░
    ██╔══╝░░██║░░░██║░░░██║░░░██║░░░██║██╔══██╗██╔══╝░░
    ██║░░░░░╚██████╔╝░░░██║░░░╚██████╔╝██║░░██║███████╗
    ╚═╝░░░░░░╚═════╝░░░░╚═╝░░░░╚═════╝░╚═╝░░╚═╝╚══════╝
    ████████╗███████╗░█████╗░██╗░░██╗
    ╚══██╔══╝██╔════╝██╔══██╗██║░░██║
    ░░░██║░░░█████╗░░██║░░╚═╝███████║
    ░░░██║░░░██╔══╝░░██║░░██╗██╔══██║
    ░░░██║░░░███████╗╚█████╔╝██║░░██║
    ░░░╚═╝░░░╚══════╝░╚════╝░╚═╝░░╚═╝
    ██████╗░██╗░░░░░░█████╗░░██████╗░
    ██╔══██╗██║░░░░░██╔══██╗██╔════╝░
    ██████╦╝██║░░░░░██║░░██║██║░░██╗░
    ██╔══██╗██║░░░░░██║░░██║██║░░╚██╗
    ██████╦╝███████╗╚█████╔╝╚██████╔╝
    ╚═════╝░╚══════╝░╚════╝░░╚═════╝░
    Welcome engineer.
    https://www.future.co.jp/recruit/
  -->
  
  <title>Sentence-Transformersを使ってみた！YouTube動画のセリフを手軽にセマンティック検索 | フューチャー技術ブログ</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  
  <meta name="description" content="はじめにこんにちは。フューチャーTIG DXユニット所属の王です。 本記事は、夏の自由研究ブログ連載2023の4本目です。 今回はテキストの埋め込みエンコーダーを使ってセマンティック検索をおもちゃレベルで簡単に実装する方法を紹介します。分かりやすいように、YouTubeの動画のセリフをコーパスとして使用します。将来的に時間軸のメタ情報も利用したら、検索結果には動画の何分何秒に特定、遷移リンクの生成">
<meta property="og:type" content="article">
<meta property="og:title" content="Sentence-Transformersを使ってみた！YouTube動画のセリフを手軽にセマンティック検索 | フューチャー技術ブログ">
<meta property="og:url" content="https://future-architect.github.io/articles/20230906a/index.html">
<meta property="og:site_name" content="フューチャー技術ブログ">
<meta property="og:description" content="はじめにこんにちは。フューチャーTIG DXユニット所属の王です。 本記事は、夏の自由研究ブログ連載2023の4本目です。 今回はテキストの埋め込みエンコーダーを使ってセマンティック検索をおもちゃレベルで簡単に実装する方法を紹介します。分かりやすいように、YouTubeの動画のセリフをコーパスとして使用します。将来的に時間軸のメタ情報も利用したら、検索結果には動画の何分何秒に特定、遷移リンクの生成">
<meta property="og:locale" content="ja_JP">
<meta property="og:image" content="https://future-architect.github.io/images/20230906a/Future_Search_Semantic_HighTech.jpg">
<meta property="article:published_time" content="2023-09-05T15:00:00.000Z">
<meta property="article:modified_time" content="2023-09-06T05:23:23.662Z">
<meta property="article:tag" content="ChatGPT">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="SemanticSearch">
<meta property="article:tag" content="embedding">
<meta property="article:tag" content="SentenceTransformers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://future-architect.github.io/images/20230906a/Future_Search_Semantic_HighTech.jpg">
  
  <link rel="alternate" href="/atom.xml" title="フューチャー技術ブログ" type="application/atom+xml">
  
  <link rel="icon" href="/favicon.ico">
  <link rel="apple-touch-icon" sizes='180x180' href="/apple-touch-icon.png">
  <link rel="apple-touch-icon" sizes='57x57' href="/apple-touch-icon-57x57.png">
  <link rel="canonical" href="https://future-architect.github.io/articles/20230906a/">
  <meta content="ChatGPT,AI,SemanticSearch,embedding,SentenceTransformers" name="keywords">
  <meta content="王紹宇" name="author">
  <link rel="preload" as="image" href="/banner.jpg" />
  <link rel='manifest' href='/manifest.webmanifest'/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css" integrity="sha384-F3w7mX95PdgyTmZZMECAngseQB83DfGTowi0iMjiWaeVhAn4FJkqJByhZMI3AhiU" crossorigin="anonymous">
  <link rel="stylesheet" href="/metronic/assets/style.css">
  <link rel="stylesheet" href="/css/theme-styles.css">
<meta name="generator" content="Hexo 5.4.2"></head>

<body class="corporate">
  <div class="wrap" itemscope itemtype="https://schema.org/TechArticle">
  <!-- BEGIN HEADER -->
<header class="header">
	<div class="header-overlay">
		<div class="header-menu"></div>
		<div class="header-title"><a href="/">Future Tech Blog</a></div>
		<div class="header-title-sub">フューチャー技術ブログ</div>
	</div>
</header>
<!-- Header END -->

  <div class="container">
  <ul class="breadcrumb">
    <li><a href="/">Home</a></li>
    <li><a href="/articles/">Blog</a></li>
    <li class="active">Post</li>
  </ul>
  <section id="main" class="margin-top-30">
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/DataScience/">DataScienceカテゴリ</a>
  </div>


    <h2 itemprop="name" class="article-title">Sentence-Transformersを使ってみた！YouTube動画のセリフを手軽にセマンティック検索
  
  <a target="_blank" rel="noopener" href="https://github.com/future-architect/tech-blog/edit/master/source/_posts/20230906a_Sentence-Transformersを使ってみた！YouTube動画のセリフを手軽にセマンティック検索.md" title="Suggest Edits" class="github-edit"><i class="github-edit-icon"></i></a>
  
</h2>

    <div class="row">
  <main class="col-md-9 blog-posts">
    <article id="post-20230906a_Sentence-Transformersを使ってみた！YouTube動画のセリフを手軽にセマンティック検索" class="article article-type-post blog-item" itemscope itemprop="blogPost">
      <div class="article-inner">
        
        <header class="article-header">
          <ul class="blog-info">
            <li class="blog-info-item"><a href="/articles/2023/" class="publish-date"><time datetime="2023-09-05T15:00:00.000Z" itemprop="datePublished">2023.09.06</time></a>
</li>
            <li class="blog-info-item"><li><a href="/authors/%E7%8E%8B%E7%B4%B9%E5%AE%87" title="王紹宇さんの記事一覧へ" class="post-author">王紹宇</a></li></li>
            <li class="blog-info-item">
  
    
    <a href="/tags/ChatGPT/" title="ChatGPTタグの記事へ" class="tag-list-link">ChatGPT</a>
  
    
    <a href="/tags/AI/" title="AIタグの記事へ" class="tag-list-link">AI</a>
  
    
    <a href="/tags/SemanticSearch/" title="SemanticSearchタグの記事へ" class="tag-list-link">SemanticSearch</a>
  
    
    <a href="/tags/embedding/" title="embeddingタグの記事へ" class="tag-list-link">embedding</a>
  
    
    <a href="/tags/SentenceTransformers/" title="SentenceTransformersタグの記事へ" class="tag-list-link">SentenceTransformers</a>
  

</li>
          </ul>
          </header>
        
        <div class="article-entry" itemprop="articleBody">
          
            <h2 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h2><p>こんにちは。フューチャーTIG DXユニット所属の王です。</p>
<p>本記事は、<a href="/articles/20230830a/">夏の自由研究ブログ連載2023</a>の4本目です。</p>
<p>今回はテキストの埋め込みエンコーダーを使ってセマンティック検索をおもちゃレベルで簡単に実装する方法を紹介します。分かりやすいように、YouTubeの動画のセリフをコーパスとして使用します。将来的に時間軸のメタ情報も利用したら、検索結果には動画の何分何秒に特定、遷移リンクの生成などもいろいろ面白いことができると思います。</p>
<h2 id="目次"><a href="#目次" class="headerlink" title="目次"></a>目次</h2><ul>
<li>セマンティック検索を注目するきっかけ</li>
<li>セマンティック検索とは</li>
<li>原理<ul>
<li>埋め込みベクトル(Embedding Vector)</li>
<li>埋め込みベクトルを使ったセマンティック検索</li>
</ul>
</li>
<li>実装<ul>
<li>使ったライブラリ</li>
<li>Semantic Searchの実装</li>
</ul>
</li>
<li>まとめ</li>
</ul>
<h2 id="セマンティック検索を注目するきっかけ"><a href="#セマンティック検索を注目するきっかけ" class="headerlink" title="セマンティック検索を注目するきっかけ"></a>セマンティック検索を注目するきっかけ</h2><p>ChatGPTなどの生成AIが大ヒットしている現在、その応用場面は増加しており、自然言語で機械と会話し指示を与えたり情報を引いたりすることは今どきのトレンドになっています。しかし、生成AIを使用する際には、情報の最新性やファクトチェックの不足などの懸念点が存在します。これらの問題を低減するためには、なるべくコンテキストや背景情報など、比較的な高品質のインプットを提供し、生成式AIが得意な情報の抽出、変換、整形などのみ任せるのがうまい使い方でしょう。</p>
<p>生成AIに１回のクエリでインプットできる情報は限られていますので、事前に関係しそうな情報を粗く抽出するために、公開していないデータや特定のコーパスを使って、自然言語でクエリする際に、セマンティック検索が必要となります。</p>
<h2 id="セマンティック検索-Semantic-Search-とは"><a href="#セマンティック検索-Semantic-Search-とは" class="headerlink" title="セマンティック検索(Semantic Search)とは"></a>セマンティック検索(Semantic Search)とは</h2><p>最初に、セマンティック検索と典型的なレキシカル検索（語彙検索、字句検索、Lexical Search）を比較します。レキシカル検索は、テキスト内の文字列や単語の表面的な一致に焦点を当てます。特定の文字列や単語がテキスト内に存在するかどうかを確認し、その一致度に基づいて情報を選別します。そのため、同義語や関連語、コンテキストに対応することが難しく、意味的な関連性を欠いた検索結果になり、自然言語のクエリに対する弱点があります。</p>
<p>一方、セマンティック検索は、キーワードだけでなく、文脈や意味に基づいて情報を検索するアプローチです。関連性やコンテキストを考慮し、より高度な情報検索を実現でき、同義語や関連語、さらに部分的な誤字などにも対応できるため、自然言語のクエリに適しています。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>テキストの埋め込みによってセマンティック検索の原理を簡単に説明します。</p>
<h3 id="埋め込みベクトル-Embedding-Vector"><a href="#埋め込みベクトル-Embedding-Vector" class="headerlink" title="埋め込みベクトル(Embedding Vector)"></a>埋め込みベクトル(Embedding Vector)</h3><p>コンピュータの世界には、文字だけではなく、画像、音声、動画などすべてのデータは符号化(Encoding)での表現ができます。それと似た思想で、単語、文、段落などを表す「意味」や「関連性」を数値のベクトルの表現で符号化に変換することは「埋め込み」と言います。その変換の条件は、意味が近い原文の変換後の埋め込みベクトルも距離が近いことです。</p>
<p>そうすることによって、統一化された表現形式「埋め込みベクトル」で「意味」の近さが定量的に表現で切るようになります。もちろん、文に対してベクトルの埋め込みは、深層学習などの技術を使って大量な事前計算が必要ですが、自ら訓練しても良いですし、後述のSentence-Transformerを利用して、公開の事前訓練された公開のモデルを簡単に使用できます。</p>
<p>ところで、ここの「距離」の定義は、ベクトルのドット積、コサイン類似度、ユークリッド距離など多数の形式はできますが、予め選定したら良いです。計算の簡単さを考慮したら、ユークリッド距離よりドット積、コサイン類似度のほうがよく採用されるでしょう。そしてベクトルを正規化（長さ1に統一する）のテクニックを使ったら、みんな等価になります。</p>
<p>さらに、もとの情報は文字に限らず、画像や音声、マルチメディアの情報も埋め込みベクトルに変換して数値化にしたら、文字と画像の距離や画像と音声の距離なども測ることが可能になります。画像や音声の類似検索、タグや説明文との紐付けなどいろいろ応用場面が可能になります。</p>
<h3 id="埋め込みベクトルを使ったセマンティック検索"><a href="#埋め込みベクトルを使ったセマンティック検索" class="headerlink" title="埋め込みベクトルを使ったセマンティック検索"></a>埋め込みベクトルを使ったセマンティック検索</h3><p>余談ですが、RDF (Resource Description Framework) を使用したセマンティック検索もありましたが、高度な事前定義と複雑のアルゴリズムが必要で実装は難しいです。今回ご紹介している埋め込みベクトルの手法は、事前のモデルの訓練での大量な計算でカバーしています。ただし、そのモデルの計算は、車輪の発明のように、大手が1度作ったら、誰でも繰り返して利用できて、恩恵を受けられます。これまで以上にAIの民主化を進めていますね。</p>
<p>さて、埋め込みベクトルを使ったセマンティック検索の手順を簡単にまとめます。</p>
<ol>
<li>事前にデータベースやコーパスの情報を文や段落粒度を分割し、それぞれ高次元(数百から数千次元)のベクトルに埋め込みエンコーディング変換しておきます。</li>
<li>クエリ文も同様に埋め込みエンコーディングして、ベクトル化して、それと距離が近いものが検索の候補結果になります。</li>
<li>計算した距離（近似度）がの検索のランキングになります。</li>
<li>（Optional）そして、検索の動作を高速化するために、事前のコーパスにベクトルによってインデックスをつけることができます。後述のSimple Neighborsはインデックスの構造と高速化検索をやってくれます。</li>
</ol>
<h2 id="実装"><a href="#実装" class="headerlink" title="実装"></a>実装</h2><h3 id="使ったライブラリ"><a href="#使ったライブラリ" class="headerlink" title="使ったライブラリ"></a>使ったライブラリ</h3><p>以下の2つのライブラリを使って実装しています。どれもシンプルなインターフェースを持って使いやすいと思います。</p>
<h4 id="Sentence-Transformers"><a href="#Sentence-Transformers" class="headerlink" title="Sentence-Transformers"></a>Sentence-Transformers</h4><p><a target="_blank" rel="noopener" href="https://sbert.net/">https://sbert.net/</a></p>
<p>pipを使用して簡単にインストールできます。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install sentence-transformers</span><br></pre></td></tr></table></figure>

<p>Sentence-Transformersは、テキストだけではなく、画像のembeddingも対応できますが、今回はテキストの検索にフォーカスしたいので割愛します。画像の検索の詳細は<a target="_blank" rel="noopener" href="https://sbert.net/examples/applications/image-search/README.html">このページ</a>をご参考ください。</p>
<p>Sentence-Transformersのフレームワークが<a target="_blank" rel="noopener" href="https://huggingface.co/sentence-transformers">huggingface</a>で多数のモデルが公開しています（執筆時点124個）。</p>
<p>モデルの命名について、<code>qa</code>がついているモデルは、(質問、回答) ペアのセットでトレーニングされて、セマンティック検索用です。つまり、クエリ&#x2F;質問が与えられた場合、関連する文章を見つける用途です。そして、<code>multi</code>がついているモデルは、多言語対応のモデルです。違う言語のインプットであっても、意味が似たものなら埋め込みベクトルの距離が近いようにエンコーディングしてくれます。ちなみに、最初から多言語のデータを使わず、例えばまずは英語で訓練して、そのモデルを教師モデルとして利用し、更に多言語に拡張する手法もあるらしく、興味深いです。</p>
<h4 id="Simple-Neighbors"><a href="#Simple-Neighbors" class="headerlink" title="Simple Neighbors"></a>Simple Neighbors</h4><p><a target="_blank" rel="noopener" href="https://simpleneighbors.readthedocs.io/en/latest/">https://simpleneighbors.readthedocs.io/en/latest/</a></p>
<p>コーパスの項目に対して最近傍検索を実行するための簡単なインターフェイスです。<br><code>Annoy</code>、<code>Sklearn</code>、<code>BruteForcePurePython</code>の3つのバックエンドをサポートしていますが、<code>Annoy</code>が推奨していますので、それも一緒にインストールします。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install simpleneighbors annoy</span><br></pre></td></tr></table></figure>

<p>高速に検索するため、事前にindexのツリーをビルドする必要があります。つまり、検索対象のデータを増加したら、改めてツイリーのビルドが必要という点に要注意です。</p>
<p>また、N-Neighborを探す結果は近似的な結果になることにもご注意ください。とはいえ、訓練のモデルから検索結果の精度はすべて有限であるので、近似と言っても十分な精度が保証できていると思います。（参考: <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximate_nearest_neighbor">Approximate Nearest Neighbors</a>）</p>
<h3 id="Semantic-Searchの実装"><a href="#Semantic-Searchの実装" class="headerlink" title="Semantic Searchの実装"></a>Semantic Searchの実装</h3><p>今回はこのドキュメントを参考して、実装してみました。</p>
<p><a target="_blank" rel="noopener" href="https://www.sbert.net/examples/applications/semantic-search/README.html">https://www.sbert.net/examples/applications/semantic-search/README.html</a></p>
<p>まずは、<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=9tSEByUy47o&ab_channel=FUTURERecruiting/%E3%83%95%E3%83%A5%E3%83%BC%E3%83%81%E3%83%A3%E3%83%BC%E6%A0%AA%E5%BC%8F%E4%BC%9A%E7%A4%BE">フューチャーの会社紹介ページ</a>のYouTube動画のセリフを<code>corpus/future.txt</code>ファイルに保存します。今回は手動で前処理として文と文の間に改行で区切りました。<br>（<em>※YouTubeから自動生成のセリフで誤字などが入っています。一旦無視します。<br>ただし、「フューチャー」が「Qちゃん」になっているのはみっともないので手修正を加えました。</em>）</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">皆さん、こんにちは。</span><br><span class="line">フューチャーのWebセミナーにアクセスいただき、ありがとうございます。</span><br><span class="line">この動画ではフューチャーの会社概要とビジネスについてご紹介します。</span><br><span class="line">早速、会社概要からご紹介します。</span><br><span class="line">フューチャーは1989年にエンジニアが立ち上げたITコンサルティング企業です。</span><br><span class="line">創業時から、ITでビジネスを牽引することをコンセプトに掲げ、いわゆるDXにあたることを推進してきました。</span><br><span class="line">また、店頭公開時には日本で初めてITコンサルティング業として事業登録をされたのもフューチャーです。</span><br><span class="line">日本初のITコンサルティング企業であり、DXを30年以上推進してきた会社と覚えていただければと思います。</span><br><span class="line">業績も昨年は過去最高を更新するなど非常に順調です。</span><br><span class="line">次に、私たちが大切にしている考え方、Our Philosophyをご紹介します。</span><br><span class="line">「本質を見極める」、「大義を問う」、「初めてに挑戦する」、「難題を楽しむ」、「ないものはつくる」。</span><br><span class="line">例えば、創業当社から他の会社がなかなか手がけないような案件であったり、難しい案件に積極的にチャレンジしてきましたので、初めてに挑戦する難題を楽しむであったり、エンジニアニメが作り上げた会社というところもあり、ないものは何でも自分たちで作ってしまおう、そういったカルチャーも深く浸透しています。</span><br><span class="line">続いて、フューチャーのミッションを紹介します。</span><br><span class="line">お客様の未来活用を最大化し、自らも新たな価値を創造する。</span><br><span class="line">フューチャーグループには大きく2つの事業体があります。</span><br><span class="line">1つは、ITコンサルティング&amp;サービス事業です。</span><br><span class="line">こちらはお客様向けの課題解決をしていく事業群で、フューチャーアーキテクトがコアカンパニーとして、ITコンサルティングを牽引しています。</span><br><span class="line">もう一つはビジネスイノベーション事業です。</span><br><span class="line">こちらはこれまでのノウハウを生かして、自社でサービスを立ち上げようというもので、まさにお客様の未来活用を最大化するITコンサルティングと、自分たちでも新たな価値を創造していく両軸で事業を展開しています。</span><br><span class="line">ここからはフューチャーのビジネスについてご紹介します。</span><br><span class="line">フューチャーのお客様は、様々な業界そしてそれぞれの業界を代表するような企業様です。</span><br><span class="line">私たちは、私たちの強みであるITを用いて、それぞれのお客様の経営課題を解決したり、あるいはお客様と一緒に業界改革をIP戦略パートナーとしてになっています。</span><br><span class="line">私たちのビジネスの特徴をご紹介します。</span><br><span class="line">創業当初から、お客様の経営戦略、それを達成するための業務改革、そしてそれを支えているシステム改革。</span><br><span class="line">これらを三位一体で捉えてプロジェクトを推進してきました。</span><br><span class="line">昨今、DXと盛んに叫ばれるようになりましたが、私たちフューチャーは経営と業務、そしてその裏にあるシステムは切っても切り離せないものだと創業当初から考えて、それらを三位一体で捉えて推進するということを30年以上続けてきました。</span><br><span class="line">さらに詳細にビジネスの流れや他社との違いについてご紹介します。</span><br><span class="line">プロジェクトはどんな未来を描くのか、戦略を立て、計画に落として、その計画に則ってシステム的に具現化し、出来上がったシステムが価値を創造するという流れが一般的です。</span><br><span class="line">プロジェクトの推進体制で見てみると、一般的には得意な領域ごとに会社が複数社にまたがって行っているケースが多いです。</span><br><span class="line">例えば、未来を描くところはコンサル系の企業様が行って、具現化していくところはSIer系の企業様が行ってといった形です。</span><br><span class="line">あるいは、一つの会社であるけれど、コンサルタントとエンジニアのように職種が分かれているケースも多いかなと思います。</span><br><span class="line">では、フューチャーはどうかと言いますと、フィーチャーは図の通り、戦略からシステム構築運用まで一気通貫でになっています。</span><br><span class="line">また、職種もITコンサルタント職一触者です。</span><br><span class="line">戦略を立てるコンサルタントとしての部分と、システムを構築していくエンジニアとしての部分、どちらも一人一人のITコンサルタントが担っています。</span><br><span class="line">ソースコードレベルで相手を理解しているITコンサルタントが担うからこそ、絵に描いた餅で終わるというのではなくて、しっかりと価値を想像するところまで伴走できる。</span><br><span class="line">そしてそれを30年以上続けてきたというのはなかなか他の会社には簡単に真似できないフューチャーならではの強みになっています。</span><br><span class="line">また、皆さんのキャリアというのを考えてみていただいても、コンサルタントとエンジニア、どちらも一つの会社で経験できるというのはキャリアの幅が広がり、市場価値の高い人材に成長できると思っていただけると思います。</span><br><span class="line">最後に、フューチャーのことをもっと知りたい方に各種メディアをご紹介します。</span><br><span class="line">フューチャーのオウンドメディア未来報では、フューチャーの人に焦点を当ててキャリアやカルチャーをご紹介しています。</span><br><span class="line">フューチャーが大切にしている技術についてもっと深く知りたい方は、テックブログやテックキャストがおすすめです。</span><br><span class="line">最後までご覧いただき、ありがとうございます。</span><br><span class="line">皆さんと選考でお会いできることを楽しみにしております。</span><br></pre></td></tr></table></figure>

<p>今回は<a target="_blank" rel="noopener" href="https://www.sbert.net/docs/pretrained_models.html">このページ</a>に紹介したモデルの中に、multi言語対応のモデルをピックアップし、予めメタデータとして用意します。モデル名<code>name</code>、ベクトルの次元<code>dims</code>、距離関数<code>metric</code>の属性を定義します。方便上、名前でモデルを引く関数<code>find_model_with_name</code>も定義します。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">models = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># Multi-lingual model of Universal Sentence Encoder for 15 languages:</span></span><br><span class="line">        <span class="comment"># Arabic, Chinese, Dutch, English, French, German, Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish.</span></span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;distiluse-base-multilingual-cased-v1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;dims&quot;</span>: <span class="number">512</span>,</span><br><span class="line">        <span class="string">&quot;metric&quot;</span>: <span class="string">&quot;angular&quot;</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># Multi-lingual model of Universal Sentence Encoder for 50 languages.</span></span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;distiluse-base-multilingual-cased-v2&quot;</span>,</span><br><span class="line">        <span class="string">&quot;dims&quot;</span>: <span class="number">512</span>,</span><br><span class="line">        <span class="string">&quot;metric&quot;</span>: <span class="string">&quot;angular&quot;</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># Multi-lingual model of paraphrase-multilingual-MiniLM-L12-v2, extended to 50+ languages.</span></span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;paraphrase-multilingual-MiniLM-L12-v2&quot;</span>,</span><br><span class="line">        <span class="string">&quot;dims&quot;</span>: <span class="number">384</span>,</span><br><span class="line">        <span class="string">&quot;metric&quot;</span>: <span class="string">&quot;angular&quot;</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># Multi-lingual model of paraphrase-mpnet-base-v2, extended to 50+ languages.</span></span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;paraphrase-multilingual-mpnet-base-v2&quot;</span>,</span><br><span class="line">        <span class="string">&quot;dims&quot;</span>: <span class="number">768</span>,</span><br><span class="line">        <span class="string">&quot;metric&quot;</span>: <span class="string">&quot;angular&quot;</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># This model was tuned for semantic search:</span></span><br><span class="line">        <span class="comment"># Given a query/question, if can find relevant passages.</span></span><br><span class="line">        <span class="comment"># It was trained on a large and diverse set of (question, answer) pairs.</span></span><br><span class="line">        <span class="comment"># 215M (question, answer) pairs from diverse sources.</span></span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;multi-qa-mpnet-base-dot-v1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;dims&quot;</span>: <span class="number">768</span>,</span><br><span class="line">        <span class="string">&quot;metric&quot;</span>: <span class="string">&quot;dot&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># This model was tuned for semantic search:</span></span><br><span class="line">        <span class="comment"># Given a query/question, if can find relevant passages.</span></span><br><span class="line">        <span class="comment"># It was trained on a large and diverse set of (question, answer) pairs.</span></span><br><span class="line">        <span class="comment"># 215M (question, answer) pairs from diverse sources.</span></span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;multi-qa-mpnet-base-cos-v1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;dims&quot;</span>: <span class="number">768</span>,</span><br><span class="line">        <span class="string">&quot;metric&quot;</span>: <span class="string">&quot;angular&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_model_with_name</span>(<span class="params">models, name</span>):</span><br><span class="line">    <span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">        <span class="keyword">if</span> model[<span class="string">&quot;name&quot;</span>] == name:</span><br><span class="line">            <span class="keyword">return</span> model</span><br><span class="line">    <span class="keyword">raise</span> NameError(<span class="string">f&quot;Could not find model <span class="subst">&#123;name&#125;</span>.&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>以下はSemanticSearchクラスでシンプルにベーシックな機能（モデルを読み込み、corpusの読み込み、エンコードして文をベクトル化すし、vector tree indexのビルド、そして、N個の最近傍探索）を実装します。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer, util</span><br><span class="line"><span class="keyword">from</span> simpleneighbors <span class="keyword">import</span> SimpleNeighbors</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SemanticSearch</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.encoder = SentenceTransformer(model[<span class="string">&quot;name&quot;</span>])</span><br><span class="line">        self.index = SimpleNeighbors(model[<span class="string">&quot;dims&quot;</span>], model[<span class="string">&quot;metric&quot;</span>])</span><br><span class="line">        <span class="keyword">if</span> model[<span class="string">&quot;metric&quot;</span>] == <span class="string">&quot;angular&quot;</span>:</span><br><span class="line">            self.metric_func = util.cos_sim</span><br><span class="line">        <span class="keyword">elif</span> model[<span class="string">&quot;metric&quot;</span>] == <span class="string">&quot;dot&quot;</span>:</span><br><span class="line">            self.metric_func = util.dot_score</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_corpus</span>(<span class="params">self, filename</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&quot;corpus/<span class="subst">&#123;filename&#125;</span>&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            self.feed(f.read().split(<span class="string">&quot;\n&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feed</span>(<span class="params">self, sentences</span>):</span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">            vector = self.encoder.encode(sentence)</span><br><span class="line">            self.index.add_one(sentence, vector)</span><br><span class="line">        self.index.build()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">find_nearest</span>(<span class="params">self, query, n=<span class="number">5</span></span>):</span><br><span class="line">        vector = self.encoder.encode(query)</span><br><span class="line">        nearests = self.index.nearest(vector, n)</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> neighbor <span class="keyword">in</span> nearests:</span><br><span class="line">            dist = self.metric_func(vector, self.index.vec(neighbor))</span><br><span class="line">            res.append((neighbor, <span class="built_in">float</span>(dist)))</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<p>早速、クエリを投げてみます。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    model = find_model_with_name(</span><br><span class="line">        models, <span class="string">&quot;distiluse-base-multilingual-cased-v2&quot;</span>)</span><br><span class="line">    ss = SemanticSearch(model)</span><br><span class="line">    ss.load_corpus(<span class="string">&quot;future.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    res = ss.find_nearest(<span class="string">&quot;フューチャーはいつ創立されましたか。&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> res:</span><br><span class="line">        <span class="built_in">print</span>(r)</span><br></pre></td></tr></table></figure>

<p><strong>出力結果1</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;フューチャーは1989年にエンジニアが立ち上げたITコンサルティング企業です。&#x27;, 0.2547425627708435)</span><br><span class="line">(&#x27;では、フューチャーはどうかと言いますと、フィーチャーは図の通り、戦略からシステム構築運用まで一気通貫でになっています。&#x27;, 0.19687587022781372)</span><br><span class="line">(&#x27;創業時から、ITでビジネスを牽引することをコンセプトに掲げ、いわゆるDXにあたることを推進してきました。&#x27;, 0.1668681502342224)</span><br><span class="line">(&#x27;フューチャーグループには大きく2つの事業体があります。&#x27;, 0.164341002702713)</span><br><span class="line">(&#x27;昨今、DXと盛んに叫ばれるようになりましたが、私たちフューチャーは経営と業務、そしてその裏にあるシステムは切っても切り離せないものだと創業当初から考えて、それらを三位一体で捉えて推進するということを30年以上続けてきました。&#x27;, 0.16331210732460022)</span><br></pre></td></tr></table></figure>

<p>文章に「創立」などのキーワードが登場していないですけど、1個目近似度高い文（時間に関して述べているからかもしれません）がうまくヒットしています。</p>
<p>今度は他のモデルでやってみます。<br>モデル：<code>paraphrase-multilingual-MiniLM-L12-v2</code><br>クエリ：<code>フューチャーはいつ創立されましたか。</code><br><strong>出力結果2</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;昨今、DXと盛んに叫ばれるようになりましたが、私たちフューチャーは経営と業務、そしてその裏にあるシステムは切っても切り離せないものだと創業当初から考えて、それらを三位一体で捉えて推進するということを30年以上続けてきました。&#x27;, 0.45087340474128723)</span><br><span class="line">(&#x27;日本初のITコンサルティング企業であり、DXを30年以上推進してきた会社と覚えていただければと思います。&#x27;, 0.3921096622943878)</span><br><span class="line">(&#x27;フューチャーは1989年にエンジニアが立ち上げたITコンサルティング企業です。&#x27;, 0.36329418420791626)</span><br><span class="line">(&#x27;創業当初から、お客様の経営戦略、それを達成するための業務改革、そしてそれを支えているシステム改革。&#x27;, 0.3592120409011841)</span><br><span class="line">(&#x27;創業時から、ITでビジネスを牽引することをコンセプトに掲げ、いわゆるDXにあたることを推進してきました。&#x27;, 0.35177189111709595)</span><br></pre></td></tr></table></figure>

<p>結果が変わりましたが、「昨今」や「創業」や「30年」が含まれた文はトップになっています。まあまあ許容できる結果でしょう。</p>
<p>他の質問とモデルでもやってみます。</p>
<p>モデル：<code>paraphrase-multilingual-MiniLM-L12-v2</code><br>クエリ：<code>未来報はなんですか。</code></p>
<p><strong>出力結果3</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;フューチャーのオウンドメディア未来報では、フューチャーの人に焦点を当ててキャリアやカルチャーをご紹介しています。&#x27;, 0.5177506804466248)</span><br><span class="line">(&#x27;最後に、フューチャーのことをもっと知りたい方に各種メディアをご紹介します。&#x27;, 0.44624844193458557)</span><br><span class="line">(&#x27;プロジェクトはどんな未来を描くのか、戦略を立て、計画に落として、その計画に則ってシステム的に具現化し、出来上がったシステムが価値を創造するという流れが一般的です。&#x27;, 0.4249690771102905)</span><br><span class="line">(&#x27;例えば、未来を描くところはコンサル系の企業様が行って、具現化していくところはSIer系の企業様が行ってといった形です。&#x27;, 0.40904152393341064)</span><br><span class="line">(&#x27;次に、私たちが大切にしている考え方、Our&#x27;, 0.40697067975997925)</span><br></pre></td></tr></table></figure>

<p>モデル：<code>multi-qa-mpnet-base-dot-v1</code><br>クエリ：<code>長所はなに</code></p>
<p><strong>出力結果4</strong></p>
<p>（<em>※このモデルは、他のコサイン類似度とは違ってドット積で距離を評価しているので、1以上の距離結果がありうる</em>）</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;そしてそれを30年以上続けてきたというのはなかなか他の会社には簡単に真似できないフューチャーならではの強みになっています。&#x27;, 18.705921173095703)</span><br><span class="line">(&#x27;さらに詳細にビジネスの流れや他社との違いについてご紹介します。&#x27;, 18.43102264404297)</span><br><span class="line">(&#x27;皆さん、こんにちは。&#x27;, 16.867801666259766)</span><br><span class="line">(&#x27;業績も昨年は過去最高を更新するなど非常に順調です。&#x27;, 16.38519287109375)</span><br><span class="line">(&#x27;お客様の未来活用を最大化し、自らも新たな価値を創造する。&#x27;, 15.685336112976074)</span><br></pre></td></tr></table></figure>

<p>今度は、英語のコーパスを利用して、日本語で質問してみます。<br>HuggingFace出品の「Text embeddings &amp; semantic search」を紹介する<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=OATCgQtNX2o&ab_channel=HuggingFace">このビデオ</a>のセリフを引っ張ってきます。<code>corpus/semantic_search.txt</code>に保存します。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Text embeddings and semantic search.</span><br><span class="line">In this video we’ll explore how Transformer models represent text as embedding vectors and how these vectors can be used to find similar documents in a corpus.</span><br><span class="line">Text embeddings are just a fancy way of saying that we can represent text as an array of numbers called a vector.</span><br><span class="line">To create these embeddings we usually use an encoder-based model like BERT.</span><br><span class="line">In this example, you can see how we feed three sentences to the encoder and get three vectors as the output.</span><br><span class="line">Reading the text, we can see that walking the dog seems to be most similar to walking the cat, but let&#x27;s see if we can quantify this.</span><br><span class="line">The trick to do the comparison is to compute a similarity metric between each pair of embedding vectors.</span><br><span class="line">These vectors usually live in a high-dimensional space, so a similarity metric can be anything that measures some sort of distance between vectors.</span><br><span class="line">One popular metric is cosine similarity, which uses the angle between two vectors to measure how close they are.</span><br><span class="line">In this example, our embedding vectors live in 3D and we can see that the orange and grey vectors are close to each other and have a smaller angle.</span><br><span class="line">Now one problem we have to deal with is that Transformer models like BERT will actually return one embedding vector per token.</span><br><span class="line">For example in the sentence &quot;I took my dog for a walk&quot;, we can expect several embedding vectors, one for each word.</span><br><span class="line">For example, here we can see the output of our model has produced 9 embedding vectors per sentence, and each vector has 384 dimensions.</span><br><span class="line">But what we really want is a single embedding vector for the whole sentence.</span><br><span class="line">To deal with this, we can use a technique called pooling.</span><br><span class="line">The simplest pooling method is to just take the token embedding of the CLS token.</span><br><span class="line">Alternatively, we can average the token embeddings which is called mean pooling.</span><br><span class="line">With mean pooling only thing we need to make sure is that we don&#x27;t include the padding tokens in the average, which is why you can see the attention mask being used here.</span><br><span class="line">This now gives us one 384 dimensional vector per sentence which is exactly what we want.</span><br><span class="line">And once we have our sentence embeddings, we can compute the cosine similarity for each pair of vectors.</span><br><span class="line">In this example we use the function from scikit-learn and you can see that the sentence &quot;I took my dog for a walk&quot; has an overlap of 0.83 with &quot;I took my cat for a walk&quot;. Hooray.</span><br><span class="line">We can take this idea one step further by comparing the similarity between a question and a corpus of documents.</span><br><span class="line">For example, suppose we embed every post in the Hugging Face forums.</span><br><span class="line">We can then ask a question, embed it, and check which forum posts are most similar.</span><br><span class="line">This process is often called semantic search, because it allows us to compare queries with context.</span><br><span class="line">To create a semantic search engine is quite simple in Datasets.</span><br><span class="line">First we need to embed all the documents.</span><br><span class="line">In this example, we take a small sample from the SQUAD dataset and apply the same embedding logic as before.</span><br><span class="line">This gives us a new column called &quot;embeddings&quot; that stores the embedding of every passage.</span><br><span class="line">Once we have our embeddings, we need a way to find nearest neighbours to a query.</span><br><span class="line">Datasets provides a special object called a FAISS index that allows you to quickly compare embedding vectors.</span><br><span class="line">So we add the FAISS index, embed a question and voila. we&#x27;ve now found the 3 most similar articles which might store the answer.</span><br></pre></td></tr></table></figure>

<p>同じように、それをロードして、日本語のクエリで投げてみます。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    model = find_model_with_name(</span><br><span class="line">        models, <span class="string">&quot;paraphrase-multilingual-MiniLM-L12-v2&quot;</span>)</span><br><span class="line">    ss = SemanticSearch(model)</span><br><span class="line">    ss.load_corpus(<span class="string">&quot;semantic_search.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    res = ss.find_nearest(<span class="string">&quot;埋め込みベクトルでのエンコーディングについて、どんなモデルを使えますか&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> res:</span><br><span class="line">        <span class="built_in">print</span>(r)</span><br></pre></td></tr></table></figure>

<p><strong>出力結果5</strong></p>
<p>それなりにいい感じにヒットできていますね。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;To create these embeddings we usually use an encoder-based model like BERT.&#x27;, 0.6005619764328003)</span><br><span class="line">(&#x27;In this video we’ll explore how Transformer models represent text as embedding vectors and how these vectors can be used to find similar documents in a corpus.&#x27;, 0.5864262580871582)</span><br><span class="line">(&#x27;For example, here we can see the output of our model has produced 9 embedding vectors per sentence, and each vector has 384 dimensions.&#x27;, 0.5198760032653809)</span><br><span class="line">(&#x27;In this example, we take a small sample from the SQUAD dataset and apply the same embedding logic as before.&#x27;, 0.4749892055988312)</span><br><span class="line">(&#x27;In this example, our embedding vectors live in 3D and we can see that the orange and grey vectors are close to each other and have a smaller angle.&#x27;, 0.46906405687332153)</span><br></pre></td></tr></table></figure>

<p>モデル：<code>distiluse-base-multilingual-cased-v1</code><br>クエリ：<code>セマンティック検索には、どんなテクニックが使えるか</code></p>
<p><strong>出力結果6</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;Text embeddings and semantic search.&#x27;, 0.3169878125190735)</span><br><span class="line">(&#x27;To create a semantic search engine is quite simple in Datasets.&#x27;, 0.22516131401062012)</span><br><span class="line">(&#x27;To deal with this, we can use a technique called pooling.&#x27;, 0.19742435216903687)</span><br><span class="line">(&#x27;This process is often called semantic search, because it allows us to compare queries with context.&#x27;, 0.1717163324356079)</span><br><span class="line">(&#x27;Once we have our embeddings, we need a way to find nearest neighbours to a query.&#x27;, 0.1544724851846695)</span><br></pre></td></tr></table></figure>

<h2 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h2><p>本記事では、セマンティック検索の概念や原理を簡単に説明しました。そして埋め込みベクトルの実装をシンプルに実現してデモしました。言語問わずにクエリを投げて、そこそこの精度の検索ランキングの結果が得ました。</p>
<p>AIの民主化が発展している現在、いろいろの技術のハードルが下がってきて、中小企業や一般の人々にも簡単に利用・導入可能になり、そのオポテュニティーをうまく掴める組織と人間こそ未来の勝者になるでしょう。</p>
<p>では、ようこそ〜　Futureへ！</p>
<img src="/images/20230906a/Future_Search_Semantic_HighTech.jpg" alt="Future_Search_Semantic_HighTech.jpg" width="768" height="768" loading="lazy">
*Image Generated by leonardo.ai*

          
        </div>
        <footer>
          <section class="social-area">
          <!-- シェアボタン START -->
  <ul class="social-button">
    
    <!-- Twitter -->
    <li>
      <a class="social-btn twitter-btn" target="_blank" href="https://twitter.com/share?url=https://future-architect.github.io/articles/20230906a/&related=twitterapi%2Ctwitter&text=Sentence-Transformers%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%9F%EF%BC%81YouTube%E5%8B%95%E7%94%BB%E3%81%AE%E3%82%BB%E3%83%AA%E3%83%95%E3%82%92%E6%89%8B%E8%BB%BD%E3%81%AB%E3%82%BB%E3%83%9E%E3%83%B3%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E6%A4%9C%E7%B4%A2%20%7C%20%E3%83%95%E3%83%A5%E3%83%BC%E3%83%81%E3%83%A3%E3%83%BC%E6%8A%80%E8%A1%93%E3%83%96%E3%83%AD%E3%82%B0" rel="nofollow noopener">
        <i></i><span class="social-btn-label">ツイート</span>
      </a>
    </li>
    <!-- Facebook -->
    <li>
      <a class="social-btn fb-btn" target="_blank" href="http://www.facebook.com/share.php?u=https://future-architect.github.io/articles/20230906a/&t=Sentence-Transformers%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%9F%EF%BC%81YouTube%E5%8B%95%E7%94%BB%E3%81%AE%E3%82%BB%E3%83%AA%E3%83%95%E3%82%92%E6%89%8B%E8%BB%BD%E3%81%AB%E3%82%BB%E3%83%9E%E3%83%B3%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E6%A4%9C%E7%B4%A2" rel="nofollow noopener">
        <i></i><span class="social-btn-label">シェア</span>
      </a>
    </li>
    <!-- hatebu -->
    <li>
      <a class="social-btn hatebu-btn" target="_blank" href="https://b.hatena.ne.jp/entry/s/future-architect.github.io/articles/20230906a/" rel="nofollow noopener">
        <i></i><span class="social-btn-label">1</span>
      </a>
    </li>
    <!-- pocket -->
    <li>
      <a class="social-btn pocket-btn" target="_blank" href="https://getpocket.com/save?url=https://future-architect.github.io/articles/20230906a/" rel="nofollow noopener">
        <i></i><span class="social-btn-label">Pocket</span>
      </a>
    </li>
    
  </ul>
<!-- シェアボタン END -->

          </section>
          <aside>
            <section class="related-post margin-bottom-40 nav">
              <h2 id="related"><a href="#related" class="headerlink" title="関連記事"></a>関連記事</h2>
              
  <div class="widget">
    <ul class="nav related-post-link"><li class="related-posts-item"><span>2023.05.23</span><span class="snscount">&#9825;9</span><a href=/articles/20230523a/ title="IT業界に身を置いていると、今の技術トレンドが気になるときがありますよね。業界の潮流を把握する方法の1つとしてTechnology Radarという文書があるので、今回紹介します。">Technology Radar の機械学習関連技術を見てみる</a></li><li class="related-posts-item"><span>2023.05.01</span><span class="snscount">&#9825;1</span><a href=/articles/20230501a/ title="言語処理学会第29回年次大会 (NLP2023) に参加してきましたのでご報告いたします。当社はゴールドスポンサーとして参加し、総勢8名、オンサイトでの聴講を行いました。">言語処理学会 (NLP2023) 参加報告</a></li><li class="related-posts-item"><span>2023.04.04</span><span class="snscount">&#9825;14</span><a href=/articles/20230404a/ title="世界中で話題となっているChatGPTですが、その開発元であるOpenAIから大規模言語モデル(LLM)が労働市場に与える影響について分析した論文が公開されました。">ChatGPTなどの大規模言語モデルが労働市場に与える影響の分析</a></li><li class="related-posts-item"><span>2022.09.07</span><span class="snscount">&#9825;13</span><a href=/articles/20220907a/ title="2021年のクリスマスで発表したフューチャー Advent Calendar 2021で話した、汎用的にパズルのソルバーを実装してみたの後編として、パズルの解をどう効率的に保存する方法についての検討です。">効率的にツイスティパズルの結果を保存する方法の探求</a></li><li class="related-posts-item"><span>2020.08.10</span><span class="snscount">&#9825;52</span><a href=/articles/20200810/ title="夏自由の研究では大人たちが大好きな刺激的な「おカネ」の話をしようと思います。個人プロジェクトとして、IT技術、数学と金融の知識を融合し、FX[バイナリオプション]の自動売買ツールを作ります。">自動売買ツールを自作してみよう</a></li><li class="related-posts-item"><span>2019.06.10</span><span class="snscount">&#9825;16</span><a href=/articles/20190610/ title="若手中心の開発チームで1~2年ほど過ごしました。その時の経験を通して、こうすればもっとソースコードが綺麗になると感じたことを3点まとめます。">ソースコードを綺麗にするためにまず心がけたい３点</a></li></ul>
  </div>
            </section>
            <section class="reference-post margin-bottom-40 nav">
              
  <div class="card">
    <div id="reference" class="reference-lede"><a href="#reference" class="headerlink" title="参照されている記事"></a>この記事を参照している記事</div>
    <ul class="reference-post-link"><li class="reference-posts-item"><a href=/articles/20230830a/ title="今年も夏の自由研究連載を始めますよ記事です。皆さんも一度は通ったことのある小学生の自由研究">夏の自由研究連載2023 を始めます<span class="newitem">NEW</span></a></li></ul>
  </div>
            </section>
          </aside>
        </footer>
      </div>
    </article>
  </main>
  <aside class="col-md-3 blog-sidebar">
    <!-- START SIDEBAR  -->


<section class="toc-section">
  <h2 class="margin-top-30">目次</h2>
  <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"><span class="toc-text">はじめに</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%AC%A1"><span class="toc-text">目次</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%82%BB%E3%83%9E%E3%83%B3%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E6%A4%9C%E7%B4%A2%E3%82%92%E6%B3%A8%E7%9B%AE%E3%81%99%E3%82%8B%E3%81%8D%E3%81%A3%E3%81%8B%E3%81%91"><span class="toc-text">セマンティック検索を注目するきっかけ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%82%BB%E3%83%9E%E3%83%B3%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E6%A4%9C%E7%B4%A2-Semantic-Search-%E3%81%A8%E3%81%AF"><span class="toc-text">セマンティック検索(Semantic Search)とは</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-text">原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%8B%E3%82%81%E8%BE%BC%E3%81%BF%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB-Embedding-Vector"><span class="toc-text">埋め込みベクトル(Embedding Vector)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%8B%E3%82%81%E8%BE%BC%E3%81%BF%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E3%82%BB%E3%83%9E%E3%83%B3%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E6%A4%9C%E7%B4%A2"><span class="toc-text">埋め込みベクトルを使ったセマンティック検索</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9F%E8%A3%85"><span class="toc-text">実装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E3%81%A3%E3%81%9F%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%AA"><span class="toc-text">使ったライブラリ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sentence-Transformers"><span class="toc-text">Sentence-Transformers</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Simple-Neighbors"><span class="toc-text">Simple Neighbors</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Semantic-Search%E3%81%AE%E5%AE%9F%E8%A3%85"><span class="toc-text">Semantic Searchの実装</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%81%BE%E3%81%A8%E3%82%81"><span class="toc-text">まとめ</span></a></li></ol>
</section>

<section class="category">
<h2 class="margin-top-30">カテゴリー</h2>
<div class="widget">
  <ul class="nav sidebar-categories margin-bottom-40">
  
  <li class=""><a href="/categories/Programming/">Programming (399)</a></li>
<li class=""><a href="/categories/Infrastructure/">Infrastructure (238)</a></li>
<li class=""><a href="/categories/Culture/">Culture (88)</a></li>
<li class=""><a href="/categories/DataScience/">DataScience (56)</a></li>
<li class=""><a href="/categories/IoT/">IoT (34)</a></li>
<li class=""><a href="/categories/DB/">DB (23)</a></li>
<li class=""><a href="/categories/DevOps/">DevOps (21)</a></li>
<li class=""><a href="/categories/Business/">Business (21)</a></li>
<li class=""><a href="/categories/%E8%AA%8D%E8%A8%BC%E8%AA%8D%E5%8F%AF/">認証認可 (20)</a></li>
<li class=""><a href="/categories/Management/">Management (15)</a></li>
<li class=""><a href="/categories/Security/">Security (13)</a></li>
<li class=""><a href="/categories/VR/">VR (12)</a></li>
<li class=""><a href="/categories/Design/">Design (11)</a></li>

  </ul>
</div>

</section>
<section class="podcast-link">
<h2 class="margin-top-30">Tech Cast</h2>

  <div class="class="widget-wrap">
  <div class="widget">
    <ul class="nav techcast">
      <li><a href="https://podcasters.spotify.com/pod/show/futuretechcast/episodes/38-AIAI-e22h1v0" title="フューチャーがお届けするポッドキャストです。#38 AIグループリーダー加藤さんに聞く「AIチームのミッションと展望」" target="_blank" rel="noopener"> #38 AIグループリーダー加藤さんに聞く「AIチームのミッションと展望」</a></li>
<li><a href="https://podcasters.spotify.com/pod/show/futuretechcast/episodes/37-e227p84" title="フューチャーがお届けするポッドキャストです。#37 自然言語処理を使った文書検索エンジンシステム開発と新規サービス検討（後編）" target="_blank" rel="noopener"> #37 自然言語処理を使った文書検索エンジンシステム開発と新規サービス検討（後編）</a></li>
<li><a href="https://podcasters.spotify.com/pod/show/futuretechcast/episodes/36-e1rdbcu" title="フューチャーがお届けするポッドキャストです。#36 自然言語処理を使った文書検索エンジンシステム開発と新規サービス検討（前編）" target="_blank" rel="noopener"> #36 自然言語処理を使った文書検索エンジンシステム開発と新規サービス検討（前編）</a></li>
    </ul>
  </div>
  </div>
  
</section>
<section class="advent-calendar">
<h2 class="margin-top-30">アドベントカレンダー</h2>
<div class="widget">
  <ul class="nav-flex">
    <li><a href="http://qiita.com/advent-calendar/2022/future" title="フューチャー Advent Calendar 2022 #Qiita" target="_blank" rel="noopener">2022年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2021/future" title="フューチャー Advent Calendar 2021 #Qiita" target="_blank" rel="noopener">2021年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2020/future" title="フューチャー Advent Calendar 2020 #Qiita" target="_blank" rel="noopener">2020年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2019/future" title="フューチャー Advent Calendar 2019 #Qiita" target="_blank" rel="noopener">2019年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2018/future" title="フューチャー Advent Calendar 2018 #Qiita" target="_blank" rel="noopener">2018年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2017/future" title="フューチャー Advent Calendar 2017 #Qiita" target="_blank" rel="noopener">2017年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2016/future" title="フューチャー Advent Calendar 2016 #Qiita" target="_blank" rel="noopener">2016年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2015/future" title="フューチャー Advent Calendar 2015 #Qiita" target="_blank" rel="noopener">2015年</a></li>
  </ul>
</div>

</section>
<!-- END SIDEBAR -->

  </aside>
</div>

  </section>
</div>

      <!-- BEGIN PRE-FOOTER -->
    <footer>
      <div class="pre-footer">
        <div class="container">
          <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-6 col-6 pre-footer-col">
              <h2>About Us</h2>
              <p>経営とITをデザインする、フューチャーの技術ブログです。業務で利用している幅広い技術について紹介します。<br /><br /><a target="_blank" rel="noopener" href="http://www.future.co.jp/">http://www.future.co.jp/</a></p>
              <div class="social-btn twitter-btn twitter-follow-btn">
                <a href="https://twitter.com/intent/follow?screen_name=future_techblog " target="_blank" rel="nofollow noopener">
                  <i></i><span class="tw-btn-label">フューチャー技術ブログをフォロー</span>
                </a>
              </div>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-4 col-4 pre-footer-col">
              <h2>Contact</h2>
              <address class="margin-bottom-40">
                <a href="https://www.future.co.jp/recruit/recruit/rec-fresh/" title="新卒採用" target="_blank" rel="noopener">新卒採用</a><br>
                <a href="https://www.future.co.jp/recruit/recruit/rec-career/" title="キャリア採用" target="_blank" rel="noopener">キャリア採用</a><br>
                <a href="https://www.future.co.jp/contact_us/" title="お問い合わせページ" target="_blank" rel="noopener">お問い合わせ</a><br>
                <a href="https://www.future.co.jp/architect/socialmediapolicy/" title="ソーシャルメディアポリシー" target="_blank" rel="noopener">メディアポリシー</a><br><br>
                <a href="mailto:techblog@future.co.jp">techblog@future.co.jp</a>
              </address>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6 col-6 pre-footer-col">
              <h2>Contents</h2>
              <a href="https://future-architect.github.io/coding-standards/" title="Future Enterprise Coding Standards" target="_blank" rel="noopener">コーディング規約</a><br>
              <a href="https://future-architect.github.io/typescript-guide/" title="仕事ですぐに使えるTypeScript" target="_blank" rel="noopener">仕事ですぐに使えるTypeScript</a><br>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-3 col-3 pre-footer-col">
              <h2>Event</h2>
              <a href="https://future.connpass.com/" title="経営とITをデザインするフューチャーの勉強会です" target="_blank" rel="noopener">connpass</a><br>
              <a href="https://www.future.co.jp/futureinsightseminar/" title="フューチャーインサイトセミナー" target="_blank" rel="noopener">Webセミナー</a><br>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-3 col-3 pre-footer-col">
              <h2>SNS</h2>
              <a href="https://github.com/future-architect" title="Future's official open source repositories" target="_blank" rel="noopener">GitHub</a><br>
              <a href="https://qiita.com/organizations/future" title="フューチャーのQiita Organizationです" target="_blank" rel="noopener">Qiita</a><br>
              <a href="https://note.future.co.jp/" title="フューチャーの公式note" target="_blank" rel="noopener">未来報</a><br>
              <a href="https://www.youtube.com/channel/UCJUSwYYd0CkGgmEKAW7QVpw" title="フューチャーYoutubeチャネル" target="_blank" rel="noopener">Youtube</a>
            </div>
          </div>
        </div>
      </div>
      <div class="footer">
        <div class="container">
          <div class="row">
            <div class="col-md-6 col-sm-6 padding-top-10">
              &copy; 2023 フューチャー技術ブログ<br>
            </div>
          </div>
        </div>
      </div>
    </footer>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-X1C28R8H0M"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-X1C28R8H0M');
  gtag('config', 'UA-74047147-1'); // 過渡期対応
</script>

  </div>
</body>
</html>
