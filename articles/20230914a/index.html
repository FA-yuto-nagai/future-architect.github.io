<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <!--
    ███████╗██╗░░░██╗████████╗██╗░░░██╗██████╗░███████╗
    ██╔════╝██║░░░██║╚══██╔══╝██║░░░██║██╔══██╗██╔════╝
    █████╗░░██║░░░██║░░░██║░░░██║░░░██║██████╔╝█████╗░░
    ██╔══╝░░██║░░░██║░░░██║░░░██║░░░██║██╔══██╗██╔══╝░░
    ██║░░░░░╚██████╔╝░░░██║░░░╚██████╔╝██║░░██║███████╗
    ╚═╝░░░░░░╚═════╝░░░░╚═╝░░░░╚═════╝░╚═╝░░╚═╝╚══════╝
    ████████╗███████╗░█████╗░██╗░░██╗
    ╚══██╔══╝██╔════╝██╔══██╗██║░░██║
    ░░░██║░░░█████╗░░██║░░╚═╝███████║
    ░░░██║░░░██╔══╝░░██║░░██╗██╔══██║
    ░░░██║░░░███████╗╚█████╔╝██║░░██║
    ░░░╚═╝░░░╚══════╝░╚════╝░╚═╝░░╚═╝
    ██████╗░██╗░░░░░░█████╗░░██████╗░
    ██╔══██╗██║░░░░░██╔══██╗██╔════╝░
    ██████╦╝██║░░░░░██║░░██║██║░░██╗░
    ██╔══██╗██║░░░░░██║░░██║██║░░╚██╗
    ██████╦╝███████╗╚█████╔╝╚██████╔╝
    ╚═════╝░╚══════╝░╚════╝░░╚═════╝░
    Welcome engineer.
    https://www.future.co.jp/recruit/
  -->
  
  <title>【LLMOps】LLMの実験管理にTruLens-Evalを使ってみた | フューチャー技術ブログ</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  
  <meta name="description" content="はじめにこんにちは、SAIG&#x2F;MLOpsチームでインターンをしている吉田です。 LLMの実験管理ツール候補として、TruLens-Evalを検証しました。合わせて、LLMの実験管理についてまとめてみました。 背景と目的LLMOpsとは近年、大規模言語モデル(LLM)の性能が飛躍的に向上し、その高度な自然言語処理能力によって様々な領域での課題解決が期待されています。LLMは文章生成、翻">
<meta property="og:type" content="article">
<meta property="og:title" content="【LLMOps】LLMの実験管理にTruLens-Evalを使ってみた | フューチャー技術ブログ">
<meta property="og:url" content="https://future-architect.github.io/articles/20230914a/index.html">
<meta property="og:site_name" content="フューチャー技術ブログ">
<meta property="og:description" content="はじめにこんにちは、SAIG&#x2F;MLOpsチームでインターンをしている吉田です。 LLMの実験管理ツール候補として、TruLens-Evalを検証しました。合わせて、LLMの実験管理についてまとめてみました。 背景と目的LLMOpsとは近年、大規模言語モデル(LLM)の性能が飛躍的に向上し、その高度な自然言語処理能力によって様々な領域での課題解決が期待されています。LLMは文章生成、翻">
<meta property="og:locale" content="ja_JP">
<meta property="og:image" content="https://future-architect.github.io/images/20230914a/top.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230914a/68747470733a2f2.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230914a/image.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230914a/68747470733a2f2_2.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230914a/star-history-202391.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230914a/コメント_2023-09-01_144334.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230914a/コメント_2023-09-01_144410.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230914a/コメント_2023-09-01_144443.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230914a/Leaderboard.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230914a/Chain_Explore.png">
<meta property="article:published_time" content="2023-09-13T15:00:00.000Z">
<meta property="article:modified_time" content="2023-10-02T00:07:27.508Z">
<meta property="article:tag" content="MLOps">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="LLMOps">
<meta property="article:tag" content="TruLens-Eval">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://future-architect.github.io/images/20230914a/top.png">
  
  <link rel="alternate" href="/atom.xml" title="フューチャー技術ブログ" type="application/atom+xml">
  
  <link rel="icon" href="/logo.svg" sizes="any" type="image/svg+xml">
  <link rel="mask-icon" href="/logo.svg" sizes="any" color="#0bd">
  <link rel="icon alternate" href="/favicon.ico">
  <link rel="apple-touch-icon" sizes='180x180' href="/apple-touch-icon.png">
  <link rel="apple-touch-icon" sizes='57x57' href="/apple-touch-icon-57x57.png">
  <link rel="canonical" href="https://future-architect.github.io/articles/20230914a/">
  <meta content="MLOps,LLM,LLMOps,TruLens-Eval" name="keywords">
  <meta content="吉田尚暉" name="author">
  <link rel="preload" as="image" href="/banner.jpg" />
  <link rel='manifest' href='/manifest.webmanifest'/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css" integrity="sha384-F3w7mX95PdgyTmZZMECAngseQB83DfGTowi0iMjiWaeVhAn4FJkqJByhZMI3AhiU" crossorigin="anonymous">
  <link rel="stylesheet" href="/metronic/assets/style.css">
  <link rel="stylesheet" href="/css/theme-styles.css">
<meta name="generator" content="Hexo 5.4.2"></head>

<body class="corporate">
  <div class="wrap" itemscope itemtype="https://schema.org/TechArticle">
  <!-- BEGIN HEADER -->
<header class="header">
	<div class="header-overlay">
		<div class="header-menu"></div>
		<div class="header-title"><a href="/">Future Tech Blog</a></div>
		<div class="header-title-sub">フューチャー技術ブログ</div>
	</div>
</header>
<!-- Header END -->

  <div class="container">
  <ul class="breadcrumb">
    <li><a href="/">Home</a></li>
    <li><a href="/articles/">Blog</a></li>
    <li class="active">Post</li>
  </ul>
  <section id="main" class="margin-top-30">
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/DataScience/">DataScienceカテゴリ</a>
  </div>


    <h2 itemprop="name" class="article-title">【LLMOps】LLMの実験管理にTruLens-Evalを使ってみた
  
  <a target="_blank" rel="noopener" href="https://github.com/future-architect/tech-blog/edit/master/source/_posts/20230914a_【LLMOps】LLMの実験管理にTruLens-Evalを使ってみた.md" title="Suggest Edits" class="github-edit"><i class="github-edit-icon"></i></a>
  
</h2>

    <div class="row">
  <main class="col-md-9 blog-posts">
    <article id="post-20230914a_【LLMOps】LLMの実験管理にTruLens-Evalを使ってみた" class="article article-type-post blog-item" itemscope itemprop="blogPost">
      <div class="article-inner">
        
        <header class="article-header">
          <ul class="blog-info">
            <li class="blog-info-item"><a href="/articles/2023/" class="publish-date"><time datetime="2023-09-13T15:00:00.000Z" itemprop="datePublished">2023.09.14</time></a>
</li>
            <li class="blog-info-item"><li><a href="/authors/%E5%90%89%E7%94%B0%E5%B0%9A%E6%9A%89" title="吉田尚暉さんの記事一覧へ" class="post-author">吉田尚暉</a></li></li>
            <li class="blog-info-item">
  
    
    <a href="/tags/MLOps/" title="MLOpsタグの記事へ" class="tag-list-link">MLOps</a>
  
    
    <a href="/tags/LLM/" title="LLMタグの記事へ" class="tag-list-link">LLM</a>
  
    
    <a href="/tags/LLMOps/" title="LLMOpsタグの記事へ" class="tag-list-link">LLMOps</a>
  
    
    <a href="/tags/TruLens-Eval/" title="TruLens-Evalタグの記事へ" class="tag-list-link">TruLens-Eval</a>
  

</li>
          </ul>
          </header>
        
        <div class="article-entry" itemprop="articleBody">
          
            <img src="/images/20230914a/top.png" alt="" width="800" height="418" loading="lazy">

<h2 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h2><p>こんにちは、SAIG&#x2F;MLOpsチームでインターンをしている吉田です。</p>
<p>LLMの実験管理ツール候補として、TruLens-Evalを検証しました。合わせて、LLMの実験管理についてまとめてみました。</p>
<h2 id="背景と目的"><a href="#背景と目的" class="headerlink" title="背景と目的"></a>背景と目的</h2><h3 id="LLMOpsとは"><a href="#LLMOpsとは" class="headerlink" title="LLMOpsとは"></a>LLMOpsとは</h3><p>近年、大規模言語モデル(LLM)の性能が飛躍的に向上し、その高度な自然言語処理能力によって様々な領域での課題解決が期待されています。LLMは文章生成、翻訳、要約、質問応答など多岐にわたるタスクにおいて驚異的な成果を示しており、その応用範囲はますます広がっています。</p>
<p>LLMOpsは、LLMを組み込んだアプリケーション開発・運用の効率化を目指すプラクティスです。アプリケーションの種類や開発規模などによってLLMOpsのワークフローは大きく変わりますが、いずれの場合もLLMの性能評価や挙動の解析のために度重なる実験が必要となります。開発過程で行われる実験を適切に管理することは、LLMアプリケーションの開発を効率化させます。</p>
<img src="/images/20230914a/68747470733a2f2.png" alt="" width="960" height="540" loading="lazy">

<p>引用：<a href="/articles/20230912a/">LLM開発のフロー</a></p>
<h3 id="LLMの実験管理"><a href="#LLMの実験管理" class="headerlink" title="LLMの実験管理"></a>LLMの実験管理</h3><p>LLMに対してタスクを実行させるためには、その処理結果を適切に評価し、モデルの性能や振る舞いについて理解する必要があります。そのため、<strong>何度も実験を繰り返すこと</strong>が想定されます。しかし、<strong>LLMの性能評価や挙動解析は複雑</strong>であり、多くの要因が絡み合うことから、適切な実験管理が必要です。実験管理により、過去の実験設計や結果を簡単にアクセスできるようにすることで、アプリケーション開発の効率を上げることを目指します。</p>
<p>実験管理はこれまでのMLでも実施されてきましたが、MLの実験管理とLLMの実験管理は下記のように異なるため、あらためてやるべきことを整理する必要があります。</p>
<ul>
<li>プロンプトエンジニアリングやファインチューニングが主な性能向上方法となること。</li>
<li>モデル性能の定量的評価が難しく、用いる評価指標の検討や人間による評価が必要となること。</li>
</ul>
<p>本記事では、<strong>プロンプトエンジニアリング</strong>における実験管理を想定します。</p>
<p>プロンプトとはLLMに与える指示や要求のことを指します。タスクに合ったプロンプトを使用することで、LLMアプリケーションの質の向上が期待できます。タスクに特化したプロンプトの例は<a target="_blank" rel="noopener" href="https://www.promptingguide.ai/jp/introduction/examples">こちら</a>にまとめられています。</p>
<p>以下の図は、プロンプトエンジニアリングにおける主な実験の流れです。</p>
<img src="/images/20230914a/image.png" alt="" width="1200" height="548" loading="lazy">

<p>プロンプトエンジニアリングのための実験管理は以下の項目を中心に管理することになると考えます。</p>
<p>(〇：TruLens-Evalで実現可能、△：LangChainと組み合わせることにより実現可能、×：現状単独では対応していないと思われる)</p>
<ul>
<li>再現可能性<ul>
<li>入力テキスト(〇)</li>
<li>出力テキスト(〇)</li>
<li>システムプロンプト(△)</li>
<li>モデルのパラメータ(〇)</li>
<li>コードのバージョン管理(×)</li>
</ul>
</li>
<li>実験詳細<ul>
<li>実行ユーザ(×)</li>
<li>タイムスタンプ(〇)</li>
<li>実行時間(〇)</li>
<li>エラー文(〇)</li>
</ul>
</li>
<li>精度の評価<ul>
<li>ルールベースの評価スコア(〇)</li>
<li>LLMによる評価スコア(〇)</li>
<li>人間による評価スコア(〇)</li>
<li>人間によるテキストレビュー(×)</li>
</ul>
</li>
</ul>
<h2 id="検証概要"><a href="#検証概要" class="headerlink" title="検証概要"></a>検証概要</h2><p>Python向けのOSSであるTruLens-Evalを使ってみました。TruLens-Evalを導入することで、LLMの実験管理に必要な情報を自動的にトラッキングし、ブラウザを通して簡単に閲覧できるようになります。また、実験ごとに評価方法を設定することで、実行と同時に評価処理を行い結果を保存してくれます。</p>
<h2 id="TruLens-Eval"><a href="#TruLens-Eval" class="headerlink" title="TruLens-Eval"></a>TruLens-Eval</h2><p>TruLensは大規模言語モデルを含むニューラルネットの開発およびモニタリングを行うためのツールセットを提供しています。TruLensは、LLMベースのアプリケーションを評価するTruLens-Evalと、ディープラーニングモデルの説明可能性を実現するTruLens-Explainの2つのツールから構成されます。今回はLLM実験管理を目的とするため、TruLens-Evalについて調査しました。</p>
<p>図はTruLens-Evalの概要について説明したものです。</p>
<img src="/images/20230914a/68747470733a2f2_2.png" alt="" width="1200" height="551" loading="lazy">

<p>引用：<a target="_blank" rel="noopener" href="https://medium.com/trulens/evaluate-and-track-your-llm-experiments-introducing-trulens-86028fe9b59a">Evaluate and Track your LLM Experiments: Introducing TruLens</a></p>
<ol>
<li>LLMアプリケーションを作成する。</li>
<li>LLMアプリケーションをTruLensに接続し、ロギングを行う。</li>
<li>feedback関数をログに追加し、LLMアプリケーションの性能を評価する。</li>
<li>TruLens dashboardを使ってレコードや評価結果LLM chainのバージョンなどを確認する。</li>
<li>一番性能の良いLLM chainを得る。</li>
</ol>
<p>昨今LLM開発が注目を集めているため、TruLens-EvalのGitHubリポジトリのスター数は現在急増しています。</p>
<img src="/images/20230914a/star-history-202391.png" alt="star-history-202391.png" width="1200" height="836" loading="lazy">

<h3 id="なぜTruLens-Evalか"><a href="#なぜTruLens-Evalか" class="headerlink" title="なぜTruLens-Evalか"></a>なぜTruLens-Evalか</h3><p>TruLens-EvalはUIの見やすさやシステムの軽量さ、GitHubのスターが増加傾向にあるOSSである点などの特徴から調査の対象としました。</p>
<p>TruLens-Evalは、LLMアプリケーションの性能に対する評価をサポートし、実験管理の一部として保存します。<strong>プロンプトエンジニアリングに必要と考えられる実験管理</strong>に主な焦点を置いたツールです。</p>
<p>タスクによって最適な評価指標は変化します。また、最適な評価指標は定まっておらず、どのような指標を用いればよいのかという検討も必要になります。TruLens-Evalは実験評価について整理されたシステムを提供しているため、<strong>評価指標の検討も円滑に進めながらより質の高いアプリケーション開発を目指す</strong>ことができると考えられます。</p>
<h3 id="TruLens-Evalでできること"><a href="#TruLens-Evalでできること" class="headerlink" title="TruLens-Evalでできること"></a>TruLens-Evalでできること</h3><ul>
<li>feedback関数を利用することにより、実験の評価を円滑に実行することができます。feedback関数とは、LLMアプリケーションが生成するテキストを分析し、LLMアプリケーションの性能をスコアリングします。</li>
<li>LLMアプリケーションを実行するだけで、feedback関数の実行やLLMの利用状況の<strong>トラッキングを自動で行い</strong>、データベースに保存することができます。</li>
<li>ブラウザから実験結果を閲覧することができ、実験結果の分析をサポートします。</li>
<li>データベースには後から情報を追加することができるため、人による評価などを管理することができます。</li>
</ul>
<p>ブラウザから閲覧できる情報について説明します。</p>
<ul>
<li>LLMをアプリケーションとして扱い、アプリケーション毎に実験管理をすることができます。一覧できる情報は次の通りです。<ul>
<li>実験回数</li>
<li>実行時間の平均</li>
<li>実行によるコスト(OpenAIモデルのような従量課金の場合)</li>
<li>合計トークン数</li>
<li>評価スコア</li>
</ul>
</li>
</ul>
<img src="/images/20230914a/コメント_2023-09-01_144334.png" alt="" width="1200" height="578" loading="lazy">

<ul>
<li>実験の詳細をテーブルとして表示できます。アプリケーション名によるフィルタリングや評価スコアによる並び替え機能などが用意されています。比較できる情報は次の通りです。<ul>
<li>入力プロンプト（現在日本語(Unicode)に未対応）</li>
<li>出力結果（現在日本語(Unicode)に未対応）</li>
<li>実行時間</li>
<li>タイムスタンプ</li>
<li>合計トークン数</li>
<li>タグ</li>
<li>評価スコア（色付き）　など</li>
</ul>
</li>
</ul>
<img src="/images/20230914a/コメント_2023-09-01_144410.png" alt="" width="1200" height="579" loading="lazy">

<ul>
<li>実験毎に詳細を確認できます。<ul>
<li>入力プロンプト</li>
<li>出力結果</li>
<li>コンポーネント毎の実行時間</li>
<li>評価スコア　など</li>
</ul>
</li>
</ul>
<img src="/images/20230914a/コメント_2023-09-01_144443.png" alt="" width="1200" height="580" loading="lazy">

<p>また、LangChainを用いた場合は、モデル毎の入出力とハイパーパラメータを確認できます。（未検証）</p>
<img src="/images/20230914a/Leaderboard.png" alt="Leaderboard.png" width="1200" height="568" loading="lazy">
<img src="/images/20230914a/Chain_Explore.png" alt="Chain_Explore.png" width="1200" height="605" loading="lazy">

<h3 id="TruLens-Eval-amp-LangChain（未検証）"><a href="#TruLens-Eval-amp-LangChain（未検証）" class="headerlink" title="TruLens-Eval &amp; LangChain（未検証）"></a>TruLens-Eval &amp; LangChain（未検証）</h3><p>TruLens-EvalはLangChainの利用をサポートしているため、複数のモデルにより構成されるLLMアプリケーションについてもトラッキングが可能となります。</p>
<p>LangChainを導入することで、<strong>モデルパラメータやシステムプロンプトの保存</strong>だけでなく、<strong>各LLMコンポーネントの評価についても同時に処理を行う</strong>ことができます。単一のLLMのみを用いる場合においても、システムプロンプトを管理できることからLangChainの利用が推奨されると考えられます。</p>
<h2 id="TruLens-Evalの利用手順"><a href="#TruLens-Evalの利用手順" class="headerlink" title="TruLens-Evalの利用手順"></a>TruLens-Evalの利用手順</h2><p>TruLens-Evalの利用手順の説明します。ここでは、システムプロンプトを含めたLLMアプリケーションの評価のため、実験を行う状況を想定します。</p>
<p>LLMアプリ(fix) → システムプロンプト(fix) → 入力プロンプト → 実行&amp;評価 → 入力プロンプト → …</p>
<h3 id="LLMアプリの関数化"><a href="#LLMアプリの関数化" class="headerlink" title="LLMアプリの関数化"></a>LLMアプリの関数化</h3><p>はじめにLLMアプリの定義を行います。文字列を引数とする、文字列を返す関数を定義します。</p>
<p>例) OpenAIのモデルを使った簡単なアプリケーション</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">llm_standalone</span>(<span class="params">prompt</span>):</span><br><span class="line">    <span class="keyword">return</span> openai.ChatCompletion.create(</span><br><span class="line">    engine=<span class="string">&quot;モデル名&quot;</span>,</span><br><span class="line">    messages=[</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;system promptの入力&quot;</span>&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;</span><br><span class="line">        ]</span><br><span class="line">    )[<span class="string">&quot;choices&quot;</span>][<span class="number">0</span>][<span class="string">&quot;message&quot;</span>][<span class="string">&quot;content&quot;</span>]</span><br></pre></td></tr></table></figure>

<h3 id="LLMアプリのラッピング化と実行"><a href="#LLMアプリのラッピング化と実行" class="headerlink" title="LLMアプリのラッピング化と実行"></a>LLMアプリのラッピング化と実行</h3><p>定義したLLMアプリにアプリ名やフィードバック関数の情報を付与する形でラッピング化します。その後作成したインスタンスでLLMアプリの実行を行い、結果を取得します。このとき、実験管理に必要な情報はトラッキングされ、データベースに保存されます。</p>
<p>例) 定義したllm_standaloneのラッピング化と実行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prompt_input=<span class="string">&quot;テストするプロンプトを入力&quot;</span></span><br><span class="line">basic_app = TruBasicApp(llm_standalone, app_id=<span class="string">&quot;アプリ名&quot;</span>, feedbacks=[フィードバック関数を選択])</span><br><span class="line">response, record = basic_app.call_with_record(prompt_input) <span class="comment"># response: 出力結果、record: データベースのレコードID</span></span><br></pre></td></tr></table></figure>

<h3 id="ブラウザから閲覧"><a href="#ブラウザから閲覧" class="headerlink" title="ブラウザから閲覧"></a>ブラウザから閲覧</h3><p>シェルで以下のコマンドを実行すると、Webサーバが立ち上がりブラウザから実験結果を閲覧することができます。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">trulens-eval</span><br></pre></td></tr></table></figure>

<h3 id="人による評価について"><a href="#人による評価について" class="headerlink" title="人による評価について"></a>人による評価について</h3><p>Truクラスを利用することで、データベースに任意の情報を追加・削除したり、Webサーバの起動などができます。そのため、後から評価の追加が可能です。これを応用することで、人による評価について検討することができます。以下は考えられる運用例です。</p>
<ul>
<li>開発者毎に実験結果に対する評価を行い、チームによるレビューとして管理する。</li>
<li>評価指標（根拠性、関連性、毒性など）毎に開発者が評価を行い、統計する。</li>
</ul>
<h2 id="feedback関数"><a href="#feedback関数" class="headerlink" title="feedback関数"></a>feedback関数</h2><p>入力プロンプトや出力結果に対して評価を行うfeedback関数を定義することで、実験の評価を自動的に行うことができます。TruLensはOpenAIやHuggingFaceのモデルを用いたfeedback関数を既に用意しているため、基礎的な評価については簡単に導入することができます。</p>
<h3 id="feedback関数の実装"><a href="#feedback関数の実装" class="headerlink" title="feedback関数の実装"></a>feedback関数の実装</h3><p>まず、Providerクラスを継承する形で、feedback関数の定義を保持するクラスを作成します。Providerクラスのメンバメソッドとしてfeedback関数を定義します。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> trulens_eval <span class="keyword">import</span> Provider, Feedback</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyFeedbacks</span>(<span class="title class_ inherited__">Provider</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">input_length</span>(<span class="params">self, <span class="built_in">input</span>: <span class="built_in">str</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + <span class="built_in">len</span>(<span class="built_in">input</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">output_length</span>(<span class="params">self, output: <span class="built_in">str</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + <span class="built_in">len</span>(output))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inout_length</span>(<span class="params">self, <span class="built_in">input</span>: <span class="built_in">str</span>, output: <span class="built_in">str</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + <span class="built_in">len</span>(<span class="built_in">input</span>) + <span class="built_in">len</span>(output))</span><br></pre></td></tr></table></figure>

<p>feedback関数は以下の形式で実装します。str型の引数を1つまたは2つとり、戻り値として0.0～1.0のfloat型で返します。ここで関数定義に使われる関数名および引数名は内部的に保存されています。特に、関数名が評価カテゴリの識別名として用いられます。また、多出力に関してもサポートされています(<a target="_blank" rel="noopener" href="https://www.trulens.org/trulens_eval/custom_feedback_functions/#multi-output-feedback-functions">参考</a>)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_feedback</span>(<span class="params">self, text1: <span class="built_in">str</span>, ...</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="comment">## 評価のための処理を書く ##</span></span><br><span class="line">    <span class="keyword">return</span> <span class="comment"># 0.0～1.0の値を返す</span></span><br></pre></td></tr></table></figure>

<p>feedback関数が実装されたProviderクラスをインスタンス化します。その後、引数に関する情報を付与する形で、feedback関数をラッピングします。また、多出力の場合はここで統計処理方法を設定できます(<a target="_blank" rel="noopener" href="https://www.trulens.org/trulens_eval/api/feedback/">詳細</a>)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_feedbacks = MyFeedbacks()</span><br><span class="line"></span><br><span class="line">output_function = Feedback(my_feedbacks.input_length).on_output()</span><br><span class="line">input_function = Feedback(my_feedbacks.output_length).on_input()</span><br><span class="line">inout_function = Feedback(my_feedbacks.inout_length).on_input_output()</span><br></pre></td></tr></table></figure>

<p>前述の「LLMアプリのラッピング化と実行」に加える形で、feedback関数を複数設定することができます。これにより、実行時に自動的にfeedback関数も実行され、結果が記録されます。また、後から実験結果に対してfeedback関数を実行することも可能です（<a target="_blank" rel="noopener" href="https://www.trulens.org/trulens_eval/custom_feedback_functions/">詳細</a>）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">basic_app = TruBasicApp(llm_standalone, app_id=<span class="string">&quot;my_function&quot;</span>, feedbacks=[input_function, output_function, inout_function])</span><br><span class="line">response, record = basic_app.call_with_record(prompt_input) <span class="comment"># response: 出力結果、record: データベースのレコードID</span></span><br></pre></td></tr></table></figure>

<h3 id="用意されているfeedback関数"><a href="#用意されているfeedback関数" class="headerlink" title="用意されているfeedback関数"></a>用意されているfeedback関数</h3><p>TruLens-EvalはいくつかのOpenAIやHuggingFaceが提供しているモデルやサービスを利用したfeedback関数をいくつか用意しています。（<a target="_blank" rel="noopener" href="https://www.trulens.org/trulens_eval/function_definitions/#model-agreement">詳細</a>）</p>
<p>ここではfeedback関数の例としていくつか紹介したいと思います。</p>
<ul>
<li>Relevance（関連性）</li>
</ul>
<p>OpenAIのChatCompletionを用いて、2つのテキスト（入力と出力）の関連性を数値評価します（スモークテストは<a target="_blank" rel="noopener" href="https://www.trulens.org/trulens_eval/pr_relevance_smoke_tests/">こちら</a>）。また、与えられたコンテキストとユーザの質問文がどの程度関連しているか判定するfeedback関数も用意されています（<a target="_blank" rel="noopener" href="https://www.trulens.org/trulens_eval/qs_relevance_smoke_tests/">qs_relevance</a>）。</p>
<p>以下はRelevanceがChatCompletionに与えるシステムプロンプトです。</p>
<figure class="highlight md"><table><tr><td class="code"><pre><span class="line">You are a RELEVANCE grader; providing the relevance of the given RESPONSE to the given PROMPT.</span><br><span class="line">Respond only as a number from 1 to 10 where 1 is the least relevant and 10 is the most relevant.</span><br><span class="line"></span><br><span class="line">A few additional scoring guidelines:</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> Long RESPONSES should score equally well as short RESPONSES.</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> Answers that intentionally do not answer the question, such as &#x27;I don&#x27;t know&#x27; and model refusals, should also be counted as the most RELEVANT.</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> RESPONSE must be relevant to the entire PROMPT to get a score of 10.</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> RELEVANCE score should increase as the RESPONSE provides RELEVANT context to more parts of the PROMPT.</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> RESPONSE that is RELEVANT to none of the PROMPT should get a score of 1.</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> RESPONSE that is RELEVANT to some of the PROMPT should get as score of 2, 3, or 4. Higher score indicates more RELEVANCE.</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> RESPONSE that is RELEVANT to most of the PROMPT should get a score between a 5, 6, 7 or 8. Higher score indicates more RELEVANCE.</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> RESPONSE that is RELEVANT to the entire PROMPT should get a score of 9 or 10.</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> RESPONSE that is RELEVANT and answers the entire PROMPT completely should get a score of 10.</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> RESPONSE that confidently FALSE should get a score of 1.</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> RESPONSE that is only seemingly RELEVANT should get a score of 1.</span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> Never elaborate.</span><br><span class="line"></span><br><span class="line">PROMPT: &#123;prompt&#125;</span><br><span class="line"></span><br><span class="line">RESPONSE: &#123;response&#125;</span><br><span class="line"></span><br><span class="line">RELEVANCE:</span><br></pre></td></tr></table></figure>

<ul>
<li>Moderation（節度）</li>
</ul>
<p>ModerationはOpenAIが提供しているAPIである<a target="_blank" rel="noopener" href="https://platform.openai.com/docs/guides/moderation/overview">openai.Moderation</a>をラッピングしたものになります。これを用いることで、差別的、暴力的、性的などの評価軸で、テキストを数値評価することができます。</p>
<h2 id="ユースケース"><a href="#ユースケース" class="headerlink" title="ユースケース"></a>ユースケース</h2><p>以上の機能を踏まえて、実験管理の使用例について検討しました。</p>
<h3 id="良いプロンプトの検討"><a href="#良いプロンプトの検討" class="headerlink" title="良いプロンプトの検討"></a>良いプロンプトの検討</h3><p>各評価指標における良いプロンプトは何か、整理して検討することに役立ちます。評価指標については、ルールベースなものからLLMを用いた評価、人による評価についても対応することができます。また、あらかじめ評価指標を決定する必要はなく、後から遡って実験結果を評価することもできます。</p>
<p>実験結果を各評価指標によって並べ替えることができます。<br><img src="/images/20230914a/コメント_2023-09-06_163018.png" alt="コメント_2023-09-06_163018.png" width="1200" height="586" loading="lazy"></p>
<p>各評価指標のヒストグラムを作成することができます。アプリケーション名によるフィルタリングができます。<br><img src="/images/20230914a/コメント_2023-09-06_163113.png" alt="コメント_2023-09-06_163113.png" width="1200" height="441" loading="lazy"></p>
<p>各アプリケーションの評価スコアの平均値を一覧できます。<br><img src="/images/20230914a/コメント_2023-09-06_163146.png" alt="コメント_2023-09-06_163146.png" width="1200" height="568" loading="lazy"></p>
<h3 id="新しい評価指標の検討"><a href="#新しい評価指標の検討" class="headerlink" title="新しい評価指標の検討"></a>新しい評価指標の検討</h3><ul>
<li>feedback関数の実装に関するテンプレートがあるため、検討したfeedback関数の確認や新たなfeedback関数の追加などが簡潔になると思われます。</li>
</ul>
<h2 id="今後に向けて"><a href="#今後に向けて" class="headerlink" title="今後に向けて"></a>今後に向けて</h2><h3 id="TruLens-Evalの懸念点"><a href="#TruLens-Evalの懸念点" class="headerlink" title="TruLens-Evalの懸念点"></a>TruLens-Evalの懸念点</h3><p>TruLens-Evalを本格導入する際は、下記懸念があるため、対応が必要かもしれません。</p>
<ul>
<li>実験結果のテーブル表示に関して、日本語(Unicode)対応していない部分がある。(ただし、issueが立っている)</li>
<li>階層的な管理ができない。<ul>
<li>Appの名前に工夫</li>
<li>モデル毎にデータベースを変える</li>
</ul>
</li>
<li>ドキュメントが少ない。公式ドキュメントの情報も十分とは言えない。(Slackのコミュニティがあるため、そこに情報があるかも)</li>
<li><strong>ブラウザUIによる情報追加がサポートされていない。</strong>（人による評価がやりづらい）</li>
<li>ユーザカスタムな実験情報を含める柔軟性がない。<ul>
<li>タグやfeedback関数を使った工夫が必要</li>
</ul>
</li>
<li>同一入力プロンプトでも揺らぎがあるため、複数回実行して統計を取りたいがサポートされていない。<ul>
<li>評価方法にLLMを利用している場合、feedback関数についても揺らぎが生じると考えられる。</li>
</ul>
</li>
</ul>
<h3 id="他の候補について"><a href="#他の候補について" class="headerlink" title="他の候補について"></a>他の候補について</h3><p>他にもLLMの実験管理として有力な候補があるので、検証していきたいです。</p>
<ul>
<li>MLflow<ul>
<li><a target="_blank" rel="noopener" href="https://www.databricks.com/jp/blog/announcing-mlflow-24-llmops-tools-robust-model-evaluation">MLflow.evaluate()</a>を拡張することで、LLMOpsに対応（未調査）</li>
</ul>
</li>
<li>Weights &amp; Biases<ul>
<li>LLMOps用に<a target="_blank" rel="noopener" href="https://docs.wandb.ai/ja/guides/prompts">W&amp;B prompt</a>を公開。</li>
<li>TruLensでサポートされているようなトラッキングシステムを利用できる。</li>
<li>MLOpsのシステムと組み合わせることでLLMのファインチューニングの管理も可能。</li>
</ul>
</li>
</ul>
<p>関連して <a href="/articles/20230919a/">Prompt Flowでプロンプト評価の管理を行う</a> という記事も公開しました。</p>

          
        </div>
        <footer>
          <section class="social-area">
          <!-- シェアボタン START -->
  <ul class="social-button">
    
    <!-- Twitter -->
    <li>
      <a class="social-btn twitter-btn" target="_blank" href="https://twitter.com/share?url=https://future-architect.github.io/articles/20230914a/&related=twitterapi%2Ctwitter&text=%E3%80%90LLMOps%E3%80%91LLM%E3%81%AE%E5%AE%9F%E9%A8%93%E7%AE%A1%E7%90%86%E3%81%ABTruLens-Eval%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%9F%20%7C%20%E3%83%95%E3%83%A5%E3%83%BC%E3%83%81%E3%83%A3%E3%83%BC%E6%8A%80%E8%A1%93%E3%83%96%E3%83%AD%E3%82%B0" rel="nofollow noopener">
        <i></i><span class="social-btn-label">ツイート</span>
      </a>
    </li>
    <!-- Facebook -->
    <li>
      <a class="social-btn fb-btn" target="_blank" href="http://www.facebook.com/share.php?u=https://future-architect.github.io/articles/20230914a/&t=%E3%80%90LLMOps%E3%80%91LLM%E3%81%AE%E5%AE%9F%E9%A8%93%E7%AE%A1%E7%90%86%E3%81%ABTruLens-Eval%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%9F" rel="nofollow noopener">
        <i></i><span class="social-btn-label">シェア</span>
      </a>
    </li>
    <!-- hatebu -->
    <li>
      <a class="social-btn hatebu-btn" target="_blank" href="https://b.hatena.ne.jp/entry/s/future-architect.github.io/articles/20230914a/" rel="nofollow noopener">
        <i></i><span class="social-btn-label">2</span>
      </a>
    </li>
    <!-- pocket -->
    <li>
      <a class="social-btn pocket-btn" target="_blank" href="https://getpocket.com/save?url=https://future-architect.github.io/articles/20230914a/" rel="nofollow noopener">
        <i></i><span class="social-btn-label">Pocket</span>
      </a>
    </li>
    
  </ul>
<!-- シェアボタン END -->

          </section>
          <aside>
            <section class="related-post margin-bottom-40 nav">
              <h2 id="related"><a href="#related" class="headerlink" title="関連記事"></a>関連記事</h2>
              
  <div class="widget">
    <ul class="nav related-post-link"><li class="related-posts-item"><span>2023.09.19</span><span class="snscount">&#9825;26</span><a href=/articles/20230919a/ title="AzureのPrompt Flowを使ってLLMに入力するプロンプト評価の管理を行います。プロンプト評価の管理を行いたい背景として...">Prompt Flowでプロンプト評価の管理を行う</a></li><li class="related-posts-item"><span>2023.09.13</span><span class="snscount">&#9825;64</span><a href=/articles/20230913a/ title="「LLM開発のためにMLOpsチームがやるべきこと」というテーマで、従来のMLOpsとの違い・ツール・構成例等について調査・整理しました。LLMとはLarge Launguage Model（大規模言語モデル）の略であり..">LLM開発のためにMLOpsチームがやるべきこと</a></li><li class="related-posts-item"><span>2023.10.11</span><span class="snscount">&#9825;2</span><a href=/articles/20231011a/ title="AzureのPrompt Flowをローカル環境で動かし、作成したフローをコードで管理する方法をご紹介します。">Prompt Flowをローカルで動かす＆コードで管理する</a></li><li class="related-posts-item"><span>2023.09.12</span><span class="snscount">&#9825;69</span><a href=/articles/20230912a/ title="昨今注目されている大規模言語モデルの開発においてMLOpsチームがやるべきことを考えるため、まずはLLM開発の流れを調査・整理しました。">LLM開発のフロー</a></li><li class="related-posts-item"><span>2023.05.23</span><span class="snscount">&#9825;10</span><a href=/articles/20230523a/ title="IT業界に身を置いていると、今の技術トレンドが気になるときがありますよね。業界の潮流を把握する方法の1つとしてTechnology Radarという文書があるので、今回紹介します。">Technology Radar の機械学習関連技術を見てみる</a></li><li class="related-posts-item"><span>2023.04.14</span><span class="snscount">&#9825;2</span><a href=/articles/20230414a/ title="データ/モデル監視ツールであるVertex AI Model MonitoringとEvidently AIを利用して両者を様々な観点で検証していきます。">Vertex AI Model MonitoringとEvidently AIで運用中のモデル・データを監視する【Output Metrics編】</a></li></ul>
  </div>
            </section>
            <section class="reference-post margin-bottom-40 nav">
              
  <div class="card">
    <div id="reference" class="reference-lede"><a href="#reference" class="headerlink" title="参照されている記事"></a>この記事を参照している記事</div>
    <ul class="reference-post-link"><li class="reference-posts-item"><a href=/articles/20230913a/ title="「LLM開発のためにMLOpsチームがやるべきこと」というテーマで、従来のMLOpsとの違い・ツール・構成例等について調査・整理しました。LLMとはLarge Launguage Model（大規模言語モデル）の略であり..">LLM開発のためにMLOpsチームがやるべきこと</a></li></ul>
  </div>
            </section>
          </aside>
        </footer>
      </div>
    </article>
  </main>
  <aside class="col-md-3 blog-sidebar">
    <!-- START SIDEBAR  -->


<section class="toc-section">
  <h2 class="margin-top-30">目次</h2>
  <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"><span class="toc-text">はじめに</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%E3%81%A8%E7%9B%AE%E7%9A%84"><span class="toc-text">背景と目的</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LLMOps%E3%81%A8%E3%81%AF"><span class="toc-text">LLMOpsとは</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM%E3%81%AE%E5%AE%9F%E9%A8%93%E7%AE%A1%E7%90%86"><span class="toc-text">LLMの実験管理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A4%9C%E8%A8%BC%E6%A6%82%E8%A6%81"><span class="toc-text">検証概要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TruLens-Eval"><span class="toc-text">TruLens-Eval</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%81%AA%E3%81%9CTruLens-Eval%E3%81%8B"><span class="toc-text">なぜTruLens-Evalか</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TruLens-Eval%E3%81%A7%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8"><span class="toc-text">TruLens-Evalでできること</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TruLens-Eval-amp-LangChain%EF%BC%88%E6%9C%AA%E6%A4%9C%E8%A8%BC%EF%BC%89"><span class="toc-text">TruLens-Eval &amp; LangChain（未検証）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TruLens-Eval%E3%81%AE%E5%88%A9%E7%94%A8%E6%89%8B%E9%A0%86"><span class="toc-text">TruLens-Evalの利用手順</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM%E3%82%A2%E3%83%97%E3%83%AA%E3%81%AE%E9%96%A2%E6%95%B0%E5%8C%96"><span class="toc-text">LLMアプリの関数化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM%E3%82%A2%E3%83%97%E3%83%AA%E3%81%AE%E3%83%A9%E3%83%83%E3%83%94%E3%83%B3%E3%82%B0%E5%8C%96%E3%81%A8%E5%AE%9F%E8%A1%8C"><span class="toc-text">LLMアプリのラッピング化と実行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E3%83%96%E3%83%A9%E3%82%A6%E3%82%B6%E3%81%8B%E3%82%89%E9%96%B2%E8%A6%A7"><span class="toc-text">ブラウザから閲覧</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E3%81%AB%E3%82%88%E3%82%8B%E8%A9%95%E4%BE%A1%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6"><span class="toc-text">人による評価について</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#feedback%E9%96%A2%E6%95%B0"><span class="toc-text">feedback関数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#feedback%E9%96%A2%E6%95%B0%E3%81%AE%E5%AE%9F%E8%A3%85"><span class="toc-text">feedback関数の実装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E6%84%8F%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8Bfeedback%E9%96%A2%E6%95%B0"><span class="toc-text">用意されているfeedback関数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%83%A6%E3%83%BC%E3%82%B9%E3%82%B1%E3%83%BC%E3%82%B9"><span class="toc-text">ユースケース</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%89%AF%E3%81%84%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88%E3%81%AE%E6%A4%9C%E8%A8%8E"><span class="toc-text">良いプロンプトの検討</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B0%E3%81%97%E3%81%84%E8%A9%95%E4%BE%A1%E6%8C%87%E6%A8%99%E3%81%AE%E6%A4%9C%E8%A8%8E"><span class="toc-text">新しい評価指標の検討</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8A%E5%BE%8C%E3%81%AB%E5%90%91%E3%81%91%E3%81%A6"><span class="toc-text">今後に向けて</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TruLens-Eval%E3%81%AE%E6%87%B8%E5%BF%B5%E7%82%B9"><span class="toc-text">TruLens-Evalの懸念点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%96%E3%81%AE%E5%80%99%E8%A3%9C%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6"><span class="toc-text">他の候補について</span></a></li></ol></li></ol>
</section>

<section class="category">
<h2 class="margin-top-30">カテゴリー</h2>
<div class="widget">
  <ul class="nav sidebar-categories margin-bottom-40">
  
  <li class=""><a href="/categories/Programming/">Programming (420)</a></li>
<li class=""><a href="/categories/Infrastructure/">Infrastructure (252)</a></li>
<li class=""><a href="/categories/Culture/">Culture (98)</a></li>
<li class=""><a href="/categories/DataScience/">DataScience (60)</a></li>
<li class=""><a href="/categories/IoT/">IoT (35)</a></li>
<li class=""><a href="/categories/DB/">DB (25)</a></li>
<li class=""><a href="/categories/DevOps/">DevOps (23)</a></li>
<li class=""><a href="/categories/%E8%AA%8D%E8%A8%BC%E8%AA%8D%E5%8F%AF/">認証認可 (21)</a></li>
<li class=""><a href="/categories/Business/">Business (21)</a></li>
<li class=""><a href="/categories/Management/">Management (17)</a></li>
<li class=""><a href="/categories/Security/">Security (15)</a></li>
<li class=""><a href="/categories/VR/">VR (13)</a></li>
<li class=""><a href="/categories/Design/">Design (11)</a></li>

  </ul>
</div>

</section>
<section class="podcast-link">
<h2 class="margin-top-30">Tech Cast</h2>

  <div class="class="widget-wrap">
  <div class="widget">
    <ul class="nav techcast">
      
    </ul>
  </div>
  </div>
  
</section>
<section class="advent-calendar">
<h2 class="margin-top-30">アドベントカレンダー</h2>
<div class="widget">
  <ul class="nav-flex">
    <li><a href="http://qiita.com/advent-calendar/2023/future" title="フューチャー Advent Calendar 2023 #Qiita" target="_blank" rel="noopener">2023年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2022/future" title="フューチャー Advent Calendar 2022 #Qiita" target="_blank" rel="noopener">2022年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2021/future" title="フューチャー Advent Calendar 2021 #Qiita" target="_blank" rel="noopener">2021年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2020/future" title="フューチャー Advent Calendar 2020 #Qiita" target="_blank" rel="noopener">2020年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2019/future" title="フューチャー Advent Calendar 2019 #Qiita" target="_blank" rel="noopener">2019年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2018/future" title="フューチャー Advent Calendar 2018 #Qiita" target="_blank" rel="noopener">2018年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2017/future" title="フューチャー Advent Calendar 2017 #Qiita" target="_blank" rel="noopener">2017年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2016/future" title="フューチャー Advent Calendar 2016 #Qiita" target="_blank" rel="noopener">2016年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2015/future" title="フューチャー Advent Calendar 2015 #Qiita" target="_blank" rel="noopener">2015年</a></li>
  </ul>
</div>

</section>
<!-- END SIDEBAR -->

  </aside>
</div>

  </section>
</div>

      <!-- BEGIN PRE-FOOTER -->
    <footer>
      <div class="pre-footer">
        <div class="container">
          <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-6 col-6 pre-footer-col">
              <h2>About Us</h2>
              <p>経営とITをデザインする、フューチャーの技術ブログです。業務で利用している幅広い技術について紹介します。<br /><br /><a target="_blank" rel="noopener" href="http://www.future.co.jp/">http://www.future.co.jp/</a></p>
              <div class="social-btn twitter-btn twitter-follow-btn">
                <a href="https://twitter.com/intent/follow?screen_name=future_techblog " target="_blank" rel="nofollow noopener">
                  <i></i><span class="tw-btn-label">フューチャー技術ブログをフォロー</span>
                </a>
              </div>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-4 col-4 pre-footer-col">
              <h2>Contact</h2>
              <address class="margin-bottom-40">
                <a href="https://www.future.co.jp/recruit/recruit/rec-fresh/" title="新卒採用" target="_blank" rel="noopener">新卒採用</a><br>
                <a href="https://www.future.co.jp/recruit/recruit/rec-career/" title="キャリア採用" target="_blank" rel="noopener">キャリア採用</a><br>
                <a href="https://www.future.co.jp/contact_us/" title="お問い合わせページ" target="_blank" rel="noopener">お問い合わせ</a><br>
                <a href="https://www.future.co.jp/architect/socialmediapolicy/" title="ソーシャルメディアポリシー" target="_blank" rel="noopener">メディアポリシー</a><br><br>
                <a href="mailto:techblog@future.co.jp">techblog@future.co.jp</a>
              </address>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6 col-6 pre-footer-col">
              <h2>Contents</h2>
              <a href="https://future-architect.github.io/coding-standards/" title="Future Enterprise Coding Standards" target="_blank" rel="noopener">コーディング規約</a><br>
              <a href="https://future-architect.github.io/typescript-guide/" title="仕事ですぐに使えるTypeScript" target="_blank" rel="noopener">仕事ですぐに使えるTypeScript</a><br>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-3 col-3 pre-footer-col">
              <h2>Event</h2>
              <a href="https://future.connpass.com/" title="経営とITをデザインするフューチャーの勉強会です" target="_blank" rel="noopener">connpass</a><br>
              <a href="https://www.future.co.jp/futureinsightseminar/" title="フューチャーインサイトセミナー" target="_blank" rel="noopener">Webセミナー</a><br>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-3 col-3 pre-footer-col">
              <h2>SNS</h2>
              <a href="https://github.com/future-architect" title="Future's official open source repositories" target="_blank" rel="noopener">GitHub</a><br>
              <a href="https://qiita.com/organizations/future" title="フューチャーのQiita Organizationです" target="_blank" rel="noopener">Qiita</a><br>
              <a href="https://note.future.co.jp/" title="フューチャーの公式note" target="_blank" rel="noopener">未来報</a><br>
              <a href="https://www.youtube.com/channel/UCJUSwYYd0CkGgmEKAW7QVpw" title="フューチャーYoutubeチャネル" target="_blank" rel="noopener">Youtube</a>
            </div>
          </div>
        </div>
      </div>
      <div class="footer">
        <div class="container">
          <div class="row">
            <div class="col-md-6 col-sm-6 padding-top-10">
              &copy; 2024 フューチャー技術ブログ<br>
            </div>
          </div>
        </div>
      </div>
    </footer>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-X1C28R8H0M"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-X1C28R8H0M');
  gtag('config', 'UA-74047147-1'); // 過渡期対応
</script>

  </div>
</body>
</html>
