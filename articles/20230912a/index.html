<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <!--
    ███████╗██╗░░░██╗████████╗██╗░░░██╗██████╗░███████╗
    ██╔════╝██║░░░██║╚══██╔══╝██║░░░██║██╔══██╗██╔════╝
    █████╗░░██║░░░██║░░░██║░░░██║░░░██║██████╔╝█████╗░░
    ██╔══╝░░██║░░░██║░░░██║░░░██║░░░██║██╔══██╗██╔══╝░░
    ██║░░░░░╚██████╔╝░░░██║░░░╚██████╔╝██║░░██║███████╗
    ╚═╝░░░░░░╚═════╝░░░░╚═╝░░░░╚═════╝░╚═╝░░╚═╝╚══════╝
    ████████╗███████╗░█████╗░██╗░░██╗
    ╚══██╔══╝██╔════╝██╔══██╗██║░░██║
    ░░░██║░░░█████╗░░██║░░╚═╝███████║
    ░░░██║░░░██╔══╝░░██║░░██╗██╔══██║
    ░░░██║░░░███████╗╚█████╔╝██║░░██║
    ░░░╚═╝░░░╚══════╝░╚════╝░╚═╝░░╚═╝
    ██████╗░██╗░░░░░░█████╗░░██████╗░
    ██╔══██╗██║░░░░░██╔══██╗██╔════╝░
    ██████╦╝██║░░░░░██║░░██║██║░░██╗░
    ██╔══██╗██║░░░░░██║░░██║██║░░╚██╗
    ██████╦╝███████╗╚█████╔╝╚██████╔╝
    ╚═════╝░╚══════╝░╚════╝░░╚═════╝░
    Welcome engineer.
    https://www.future.co.jp/recruit/
  -->
  
  <title>LLM開発のフロー | フューチャー技術ブログ</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  
  <meta name="description" content="はじめにこんにちは、SAIG&#x2F;MLOpsチームでアルバイトをしている板野・平野です。 今回は、昨今注目されている大規模言語モデル(LLM)の開発においてMLOpsチームがやるべきことを考えるため、まずはLLM開発の流れを調査・整理しました。 本記事はその内容を「LLM開発のフロー」という題目でまとめたものです。LLMを本番運用するときに考慮すべきこと、LLM開発・運用を支援するサービスや">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM開発のフロー | フューチャー技術ブログ">
<meta property="og:url" content="https://future-architect.github.io/articles/20230912a/index.html">
<meta property="og:site_name" content="フューチャー技術ブログ">
<meta property="og:description" content="はじめにこんにちは、SAIG&#x2F;MLOpsチームでアルバイトをしている板野・平野です。 今回は、昨今注目されている大規模言語モデル(LLM)の開発においてMLOpsチームがやるべきことを考えるため、まずはLLM開発の流れを調査・整理しました。 本記事はその内容を「LLM開発のフロー」という題目でまとめたものです。LLMを本番運用するときに考慮すべきこと、LLM開発・運用を支援するサービスや">
<meta property="og:locale" content="ja_JP">
<meta property="og:image" content="https://future-architect.github.io/images/20230912a/LLM_dev.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230912a/flow_from_scratch.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230912a/from_base_model.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230912a/proprietary.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230912a/prompt_eng_grnd.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230912a/embedding.png">
<meta property="og:image" content="https://future-architect.github.io/images/20230912a/grounding.png">
<meta property="article:published_time" content="2023-09-11T15:00:00.000Z">
<meta property="article:modified_time" content="2024-02-20T05:41:28.763Z">
<meta property="article:tag" content="インターン">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="インターン2023">
<meta property="article:tag" content="プロンプトエンジニアリング">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://future-architect.github.io/images/20230912a/LLM_dev.png">
  
  <link rel="alternate" href="/atom.xml" title="フューチャー技術ブログ" type="application/atom+xml">
  
  <link rel="icon" href="/logo.svg" sizes="any" type="image/svg+xml">
  <link rel="mask-icon" href="/logo.svg" sizes="any" color="#0bd">
  <link rel="icon alternate" href="/favicon.ico">
  <link rel="apple-touch-icon" sizes='180x180' href="/apple-touch-icon.png">
  <link rel="apple-touch-icon" sizes='57x57' href="/apple-touch-icon-57x57.png">
  <link rel="canonical" href="https://future-architect.github.io/articles/20230912a/">
  <meta content="インターン,LLM,インターン2023,プロンプトエンジニアリング" name="keywords">
  <meta content="板野竜也" name="author">
  <link rel="preload" as="image" href="/banner.jpg" />
  <link rel='manifest' href='/manifest.webmanifest'/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.1/dist/css/bootstrap.min.css" integrity="sha384-F3w7mX95PdgyTmZZMECAngseQB83DfGTowi0iMjiWaeVhAn4FJkqJByhZMI3AhiU" crossorigin="anonymous">
  <link rel="stylesheet" href="/metronic/assets/style.css">
  <link rel="stylesheet" href="/css/theme-styles.css">
<meta name="generator" content="Hexo 5.4.2"></head>

<body class="corporate">
  <div class="wrap" itemscope itemtype="https://schema.org/TechArticle">
  <!-- BEGIN HEADER -->
<header class="header">
	<div class="header-overlay">
		<div class="header-menu"></div>
		<div class="header-title"><a href="/">Future Tech Blog</a></div>
		<div class="header-title-sub">フューチャー技術ブログ</div>
	</div>
</header>
<!-- Header END -->

  <div class="container">
  <ul class="breadcrumb">
    <li><a href="/">Home</a></li>
    <li><a href="/articles/">Blog</a></li>
    <li class="active">Post</li>
  </ul>
  <section id="main" class="margin-top-30">
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/DataScience/">DataScienceカテゴリ</a>
  </div>


    <h2 itemprop="name" class="article-title">LLM開発のフロー
  
  <a target="_blank" rel="noopener" href="https://github.com/future-architect/tech-blog/edit/master/source/_posts/20230912a_LLM開発のフロー.md" title="Suggest Edits" class="github-edit"><i class="github-edit-icon"></i></a>
  
</h2>

    <div class="row">
  <main class="col-md-9 blog-posts">
    <article id="post-20230912a_LLM開発のフロー" class="article article-type-post blog-item" itemscope itemprop="blogPost">
      <div class="article-inner">
        
        <header class="article-header">
          <ul class="blog-info">
            <li class="blog-info-item"><a href="/articles/2023/" class="publish-date"><time datetime="2023-09-11T15:00:00.000Z" itemprop="datePublished">2023.09.12</time></a>
</li>
            <li class="blog-info-item"><li><a href="/authors/%E6%9D%BF%E9%87%8E%E7%AB%9C%E4%B9%9F" title="板野竜也さんの記事一覧へ" class="post-author">板野竜也</a></li></li>
            <li class="blog-info-item">
  
    
    <a href="/tags/インターン/" title="インターンタグの記事へ" class="tag-list-link">インターン</a>
  
    
    <a href="/tags/LLM/" title="LLMタグの記事へ" class="tag-list-link">LLM</a>
  
    
    <a href="/tags/インターン2023/" title="インターン2023タグの記事へ" class="tag-list-link">インターン2023</a>
  
    
    <a href="/tags/プロンプトエンジニアリング/" title="プロンプトエンジニアリングタグの記事へ" class="tag-list-link">プロンプトエンジニアリング</a>
  

</li>
          </ul>
          </header>
        
        <div class="article-entry" itemprop="articleBody">
          
            <h1 id="はじめに"><a href="#はじめに" class="headerlink" title="はじめに"></a>はじめに</h1><p>こんにちは、SAIG&#x2F;MLOpsチームでアルバイトをしている板野・平野です。</p>
<p>今回は、昨今注目されている大規模言語モデル(LLM)の開発においてMLOpsチームがやるべきことを考えるため、まずはLLM開発の流れを調査・整理しました。</p>
<p>本記事はその内容を「LLM開発のフロー」という題目でまとめたものです。LLMを本番運用するときに考慮すべきこと、LLM開発・運用を支援するサービスやツール・LLMシステムの構成例などについては、「<a href="/articles/20230913a/">LLM開発でMLOpsチームがやるべきこと</a>」と題して別記事でご紹介していますので、ぜひ併せてご覧ください。</p>
<p>ここでのLLM開発とは、「LLM自体の開発」および「LLMを活用したシステム開発」の両方を含みます。また、「LLM自体の開発」は学習フェーズ、「LLMを活用したシステム開発」は推論フェーズ、として記載しています。</p>
<p>本記事ではLLM開発における各フェーズのフローを解説します。</p>
<h1 id="LLM開発のフロー"><a href="#LLM開発のフロー" class="headerlink" title="LLM開発のフロー"></a>LLM開発のフロー</h1><p>LLM開発は下図のように、LLM自体の開発（<a href="#1-%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E7%94%A8%E6%84%8F%E3%81%99%E3%82%8B%E5%AD%A6%E7%BF%92">1.モデルを用意する（学習）</a>）とLLMを活用したシステム開発（<a href="#2-%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E8%83%BD%E5%8A%9B%E3%82%92%E5%BC%95%E3%81%8D%E5%87%BA%E3%81%99%E6%8E%A8%E8%AB%96">2.モデルの能力を引き出す（推論）</a>）の大きく2つのフェーズに分けられます。</p>
<img src="/images/20230912a/LLM_dev.png" alt="LLM_dev.png" width="960" height="540" loading="lazy">

<p>以下、各フェーズについて詳しく解説します。</p>
<h2 id="1-モデルを用意する（学習）"><a href="#1-モデルを用意する（学習）" class="headerlink" title="1. モデルを用意する（学習）"></a>1. モデルを用意する（学習）</h2><p>モデルを用意するには、大きく分けて以下の3つの方法があります。</p>
<ul>
<li>(A) ゼロからモデルを学習する</li>
<li>(B) 公開済みの基盤モデルから学習する</li>
<li>(C) プロプライエタリモデルを利用する</li>
</ul>
<p>※プロプライエタリモデル：ソースコードや構造等が非公開なモデルのこと。<br>（例）ChatGPT(GPT-3.5)やGPT-4等</p>
<p>どの方法を用いてLLM開発を行うべきかについては、要件に応じて決定します。<br>各方法のメリット・デメリットについては下表をご参考ください。</p>
<div class="scroll"><table>
<thead>
<tr>
<th></th>
<th>(A) ゼロからモデルを学習する</th>
<th>(B) 公開済みの基盤モデルから学習する</th>
<th>(C) プロプライエタリモデルを利用する</th>
</tr>
</thead>
<tbody><tr>
<td>メリット</td>
<td>・モデル構造を自由に選択可能<br>・使用した全データセットを把握可能</td>
<td>・基盤モデル作成のコスト不要<br>・様々な公開基盤モデルを試せる</td>
<td>・推論リソースの用意が不要<br>・汎用的なタスクにおいて高品質</td>
</tr>
<tr>
<td>デメリット</td>
<td>・学習に大量の計算リソースの用意が必要<br>・推論リソースの用意が必要</td>
<td>・基盤モデルのライセンスに依存<br>・推論リソースの用意が必要</td>
<td>・オンプレ運用ができない<br>・モデルが意図しないタイミングで更新される可能性がある</td>
</tr>
</tbody></table></div>
<p>以下、モデルを用意するための各方法の詳細を解説していきます。</p>
<h3 id="A-ゼロからモデルを学習する"><a href="#A-ゼロからモデルを学習する" class="headerlink" title="(A) ゼロからモデルを学習する"></a>(A) ゼロからモデルを学習する</h3><p>ChatGPT(GPT-3.5)と学習方法が同じInstructGPTと呼ばれるモデルの学習方法は、論文として公開されており、下図のようなフローで学習されています。</p>
<p>※論文リンクはこちらです。<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a></p>
<img src="/images/20230912a/flow_from_scratch.png" alt="flow_from_scratch" width="902" height="283" loading="lazy">

<ul>
<li>①大量のコーパスで学習<ul>
<li>未学習モデルに対して大量のコーパス（自然言語の文）で学習を行い、基盤モデルを構築</li>
<li>次に来る単語（next-token）を予測するモデルとなるため、それっぽい文章を作成できる</li>
<li>基盤モデルは、質問に答えたり対話したりする能力が乏しい</li>
</ul>
</li>
<li>②教師ありファインチューニング（SFT）<ul>
<li>基盤モデルに対して「指示文・コンテキスト・理想の回答を人力で集めたデータセット」でファインチューニングする</li>
<li>人力で集めたデータセットとは、<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja">こちら</a>にあるような指示文(instruction)、コンテキスト(input)、理想の回答(output)がセットになったもの</li>
<li>ファインチューニングして得られるモデルはSFT(Supervised Fine-Tuning)モデルという</li>
<li>SFTモデルには、質問に答えたり対話したりする等のタスク処理能力がある</li>
</ul>
</li>
<li>③人間のフィードバックによる強化学習（RLHF）<ul>
<li>人間のフィードバックによる強化学習のことをReinforcement Learning from Human Feedback(RLHF)と呼ぶ</li>
<li>SFTモデルに対して、文の品質をより向上させるためにRewardモデルを使用して強化学習を行う</li>
<li>Rewardモデルとは、文章を入力すると、その文章の評価を数値として出力してくれるモデルのこと</li>
<li>Rewardモデルは人間のフィードバック等を利用して別途作成しなければならない</li>
<li>SFTモデルと比べ、人間にとってより好ましく流暢な出力が得られるようになる</li>
</ul>
</li>
</ul>
<p>①まで完了すると、プロンプトに続く文を次々と出力するだけのモデルが完成します。</p>
<p>LLM開発に、まずこのプロセスは必須です。この時点ではまだ要約や質疑応答などのタスクに対するファインチューニングが行われていないため、要約や質疑応答等のタスクをさせようとしても返答が不安定です。基盤モデルの学習データで<code>「日本の首都は？東京です。」</code>や<code>「要約してください。&quot;&quot;&quot;&lt;要約したい文書&gt;&quot;&quot;&quot;。」</code> のようなinstructionのアノテーションが付いているものは極めて稀で、なにか特定のタスクを解かせるための学習がされていないからです。</p>
<p>②まで完了すると、質問に答えたり、対話したりする能力がついたモデルが完成します。この時点で、基本的なタスクを行うのに最低限のことは身についているため、より実用に近づきます。このプロセスは大半のケースで必要だと考えられます。</p>
<p>③まで完了すると、人間にとってより好ましい出力ができるモデルが完成します。ここまでするか否かは、要件やモデルに行わせたいタスクによって判断します。</p>
<div class="note info" style="background: #e5f8e2; padding:16px; margin:24px 12px; border-radius:8px;">
  <span class="fa fa-fw fa-check-circle"></span>

<p>ゼロからモデルを学習する場合に必要な計算リソースの参考値をご紹介します。</p>
<p>GPT-3に似たBLOOMというモデルは1760億のパラメータを持っており、<a target="_blank" rel="noopener" href="https://bigscience.huggingface.co/blog/bloom">BLOOMの発表記事</a>では、学習に<code>117日</code>掛かり、<code>300万ユーロ（約4.5億円）</code>もの金銭的コストが掛かったと述べられています。</p>
<p>ただし、LLM関連の技術の進歩は早く、学習方法を工夫することで大幅に計算コストを減らしたり、パラメータ数が小さいモデルを使っても高い精度が出せたり等、より時間やコストを抑えられるようになってきています。</p>
</div>

<h3 id="B-公開済みの基盤モデルから学習する"><a href="#B-公開済みの基盤モデルから学習する" class="headerlink" title="(B) 公開済みの基盤モデルから学習する"></a>(B) 公開済みの基盤モデルから学習する</h3><p>最近では基盤モデルがオープンソースで公開されるようになってきています。</p>
<p>公開されている基盤モデルを使用すれば、「ゼロからLLMを学習する」の学習フローにおいて大きなコストを要する「①大量のコーパスで学習」のプロセスをスキップできます。</p>
<p>公開されている基盤モデルからは、以下のようなフローで追加学習を行います。</p>
<img src="/images/20230912a/from_base_model.png" alt="from_base_model" width="899" height="280" loading="lazy">

<ul>
<li>①公開されている基盤モデルの選定<ul>
<li>商用利用可能なものは、<a target="_blank" rel="noopener" href="https://ai.meta.com/llama/">Llama 2 (ラマツー)</a>や<a target="_blank" rel="noopener" href="https://huggingface.co/rinna">rinna</a>, <a target="_blank" rel="noopener" href="https://huggingface.co/cyberagent">OpenCALM</a>などがあり様々な選択肢から選択できる<ul>
<li>※公開済みモデルの中でも、<a target="_blank" rel="noopener" href="https://rinna.co.jp/news/2023/05/20220531.html">rinna (japanese-gpt-neox-3.6b-instruction-ppo)</a>のようにファインチューニングや強化学習が行われているものもあります</li>
</ul>
</li>
<li>モデル選定には「公開済みの基盤モデルが学習に用いたデータセット」や「パラメータ数」等を参考にする</li>
</ul>
</li>
<li>②教師ありファインチューニング（SFT）<ul>
<li>※ <strong>(A)ゼロからモデルを学習する</strong>と同じ</li>
</ul>
</li>
<li>③人間のフィードバックによる強化学習（RLHF）<ul>
<li>※ <strong>(A)ゼロからモデルを学習する</strong>と同じ</li>
</ul>
</li>
</ul>
<div class="note info" style="background: #e5f8e2; padding:16px; margin:24px 12px; border-radius:8px;">
  <span class="fa fa-fw fa-check-circle"></span>

<p>公開済みの基盤モデルから学習する場合に必要な計算リソースの参考値をご紹介します。</p>
<p>ゼロから学習する場合と比べ、ファインチューニングから始める場合は効率的なファインチューニング手法(Parameter-Efficient Fine-Tuning: PEFT)が複数あります。</p>
<p><a target="_blank" rel="noopener" href="https://note.com/npaka/n/na5b8e6f749ce">こちらの事例</a>ではLoRAというPEFT手法や8ビット量子化等のメモリ削減手法を適用し、<code>VRAM使用量:23.5GB</code>で1台のGPUでファインチューニングができています。</p>
</div>

<h3 id="C-プロプライエタリモデルを利用する"><a href="#C-プロプライエタリモデルを利用する" class="headerlink" title="(C) プロプライエタリモデルを利用する"></a>(C) プロプライエタリモデルを利用する</h3><p>OpenAI等のAPI経由で公開されているモデルを利用する方法です。<br>現状、LLMを手軽に用意するにはこの方法が一番です。</p>
<img src="/images/20230912a/proprietary.png" alt="proprietary" width="757" height="257" loading="lazy">

<p>そのまま使うこともできますが、API経由でプロプライエタリモデルをファインチューニングすることもできます。<br><a target="_blank" rel="noopener" href="https://platform.openai.com/docs/guides/fine-tuning">OpenAIのドキュメント</a>によると、ファインチューニングで以下のような効果が期待できるようです。</p>
<ul>
<li>プロンプトで調整するよりも高品質な結果</li>
<li>プロンプトに収まらない程の多くの例でトレーニング可能</li>
<li>プロンプトを短くできることによるトークンの節約・レイテンシの減少</li>
</ul>
<h2 id="2-モデルの能力を引き出す（推論）"><a href="#2-モデルの能力を引き出す（推論）" class="headerlink" title="2. モデルの能力を引き出す（推論）"></a>2. モデルの能力を引き出す（推論）</h2><p>続いて、LLMを活用したシステム開発（推論フェーズ）について解説します。</p>
<p>従来のMLモデルは入出力の形式が一定で、分類や予測などの特定のタスクに活用する使い方が主流でした。<br>一方、LLMは汎用的なモデルであるため、1つのLLMモデルであらゆるタスク（質問応答、要約など）をこなすことができます。<br>このため、LLMモデルを入手したら、タスクを正しくこなせるように入力（プロンプト）を調節してモデルの能力を最大限引き出す工夫が新たに必要となりました。</p>
<img src="/images/20230912a/prompt_eng_grnd.png" alt="prompt_eng_grnd" width="861" height="383" loading="lazy">


<p>図内のバックエンドプログラムとして挙げられている<a target="_blank" rel="noopener" href="https://langchain.com/">LangChain</a>とは、モデルの能力を引き出すあらゆる部分を効率化するライブラリで、知識データへの円滑なアクセス等も実装することができます。</p>
<p>モデルの能力を最大限引き出す工夫として、大きく以下の2つの要素があります。</p>
<ul>
<li>①プロンプトエンジニアリング<ul>
<li>LLMへの「聞き方」を工夫することで質問応答や算術推論などのタスク処理能力を向上させる</li>
<li>プロンプトの中に望ましい答え方のフォーマット等を埋め込んだり、例示したりする</li>
<li>望んだ回答が返ってこない確率を減らす</li>
</ul>
</li>
<li>②グラウンディング（任意）<ul>
<li><a target="_blank" rel="noopener" href="https://book.st-hakky.com/docs/llm-grounding/">グラウンディング</a>とは、仮想世界で存在するLLM等のシステムを現実世界と繋げること<ul>
<li>現実世界にアクセスして情報検索をしたり、コマンドを発生したりする等</li>
</ul>
</li>
<li>LLM単体が知らないことでも答えられるようにするための知識データを用意する<ul>
<li>文の要約など、外部知識を要しない質問の場合は不要</li>
</ul>
</li>
<li>知識データの情報をプロンプトに付加することでLLMが知らない事も答えられるようになる</li>
<li>LLMの特徴である、嘘をつくこと（&#x3D;幻覚：Hallucination）を抑えられる</li>
<li>出力の根拠が把握できる</li>
</ul>
</li>
</ul>
<h3 id="①プロンプトエンジニアリング"><a href="#①プロンプトエンジニアリング" class="headerlink" title="①プロンプトエンジニアリング"></a>①プロンプトエンジニアリング</h3><p>プロンプトエンジニアリングの一例を以下にご紹介します。</p>
<ul>
<li>Zero-shotプロンプティング<ul>
<li>モデルに対して事前に情報や例を与えずに直接質問をすること</li>
<li>質問例）「次の文をポジティブかネガティブに分類してください」「次の文章を要約してください」「1+1&#x3D; 」等</li>
</ul>
</li>
<li>Few-shotプロンプティング<ul>
<li>モデルに対して事前にいくつかの例を提示してから質問すること</li>
<li>質問例）「1+1&#x3D;2, 3+3&#x3D;6, 4+4&#x3D; 」「みかん→フルーツ, サッカー→スポーツ, 猫→ 」等</li>
</ul>
</li>
<li>Chain-of-Thought（CoT）プロンプティング<ul>
<li>モデルに中間的な推論ステップ（思考過程）を介するように促すこと</li>
<li>「ステップバイステップで考えてみましょう。」の文言を入れるだけで良い結果が得られることがある</li>
<li>質問例）「6個のバナナを買いました。それを友達に2つ渡しました。それから3つのバナナを買って1つ食べました。残りは何個ですか？ ステップバイステップで考えてみましょう。」<br>他にもプロンプトエンジニアリングやその他の技術は多々あります。興味のある方は、<a target="_blank" rel="noopener" href="https://www.promptingguide.ai/jp">こちら</a>のサイトが参考になります。</li>
</ul>
</li>
</ul>
<h3 id="②グラウンディング（任意）"><a href="#②グラウンディング（任意）" class="headerlink" title="②グラウンディング（任意）"></a>②グラウンディング（任意）</h3><p>グラウンディングの一例として、「ベクトルデータベースに知識データを格納しておき、検索クエリから類似のテキストを上位数件分持ってくる」等が挙げられます。<br>これを実現するための大まかな手順は以下の通りです。</p>
<p>まず、以下の流れでベクトルデータベースを準備します。</p>
<img src="/images/20230912a/embedding.png" alt="embedding" width="713" height="354" loading="lazy">

<ul>
<li>文書を用意する（社内QAチャットを作りたい場合は社内文書など）</li>
<li>文書が長い場合、チャンク分けをする（短い文書&#x3D;チャンクに切り分ける）</li>
<li>テキストをチャンク毎に低次元空間に埋め込む (Embeddingを作成する &#x2F; ベクトル化する)<ul>
<li>テキストを入力として受け取り、ベクトルを出力してくれる、エンベディングモデルというモデルがある</li>
<li>そのようなモデルは様々あり、OpenAIの<a target="_blank" rel="noopener" href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings">text-embedding-ada-002</a>が高性能なモデルとして挙げられる</li>
</ul>
</li>
<li>チャンク毎に、「テキスト」と「ベクトル」の対をベクトルDBとして格納する</li>
</ul>
<p>ベクトルDBが作成できたら、以下のように利用します。</p>
<img src="/images/20230912a/grounding.png" alt="grounding" width="960" height="504" loading="lazy">

<ul>
<li>ユーザーの質問文を元に、LLMが検索クエリを生成する<ul>
<li>ユーザーの質問が単純であれば、その質問をそのままクエリとしてもよい</li>
</ul>
</li>
<li>検索クエリのテキストをベクトル化する<ul>
<li>ベクトルDBを作成するときに使用したエンベディングモデルと同じものを利用する</li>
</ul>
</li>
<li>ベクトルDBに格納されている各ベクトルと、クエリのベクトルとの類似度を計算する</li>
<li>類似度が高い上位のテキスト（チャンク）を取り出し、LLMのプロンプトに含める</li>
</ul>
<p>基本となる考え方は以上ですが、これを簡単に実現できるようにクラウドサービスやライブラリとして提供されているものもあります。</p>
<p>ツール等の詳細は後日、別記事にて解説予定です。</p>
<h1 id="まとめ"><a href="#まとめ" class="headerlink" title="まとめ"></a>まとめ</h1><p>本記事では「LLM開発のフロー」と題し、LLM自体の開発およびLLMを活用したシステム開発のフローをご紹介しました。</p>
<p>これからLLM開発を検討している方々やMLOpsに携わっている方々の参考となれば幸いです。</p>
<p>今回ご紹介した「LLM開発のフロー」を前提とした、<a href="/articles/20230913a/">LLM開発でMLOpsチームがやるべきこと</a>についてもぜひ併せてご覧ください。</p>
<p>（2023年9月19日追記）関連して <a href="/articles/20230919a/">Prompt Flowでプロンプト評価の管理を行う</a> という記事も公開しました。</p>

          
        </div>
        <footer>
          <section class="social-area">
          <!-- シェアボタン START -->
  <ul class="social-button">
    
    <!-- Twitter -->
    <li>
      <a class="social-btn twitter-btn" target="_blank" href="https://twitter.com/share?url=https://future-architect.github.io/articles/20230912a/&related=twitterapi%2Ctwitter&text=LLM%E9%96%8B%E7%99%BA%E3%81%AE%E3%83%95%E3%83%AD%E3%83%BC%20%7C%20%E3%83%95%E3%83%A5%E3%83%BC%E3%83%81%E3%83%A3%E3%83%BC%E6%8A%80%E8%A1%93%E3%83%96%E3%83%AD%E3%82%B0" rel="nofollow noopener">
        <i></i><span class="social-btn-label">ツイート</span>
      </a>
    </li>
    <!-- Facebook -->
    <li>
      <a class="social-btn fb-btn" target="_blank" href="http://www.facebook.com/share.php?u=https://future-architect.github.io/articles/20230912a/&t=LLM%E9%96%8B%E7%99%BA%E3%81%AE%E3%83%95%E3%83%AD%E3%83%BC" rel="nofollow noopener">
        <i></i><span class="social-btn-label">シェア</span>
      </a>
    </li>
    <!-- hatebu -->
    <li>
      <a class="social-btn hatebu-btn" target="_blank" href="https://b.hatena.ne.jp/entry/s/future-architect.github.io/articles/20230912a/" rel="nofollow noopener">
        <i></i><span class="social-btn-label">50</span>
      </a>
    </li>
    <!-- pocket -->
    <li>
      <a class="social-btn pocket-btn" target="_blank" href="https://getpocket.com/save?url=https://future-architect.github.io/articles/20230912a/" rel="nofollow noopener">
        <i></i><span class="social-btn-label">22</span>
      </a>
    </li>
    
  </ul>
<!-- シェアボタン END -->

          </section>
          <aside>
            <section class="related-post margin-bottom-40 nav">
              <h2 id="related"><a href="#related" class="headerlink" title="関連記事"></a>関連記事</h2>
              
  <div class="widget">
    <ul class="nav related-post-link"><li class="related-posts-item"><span>2023.09.13</span><span class="snscount">&#9825;67</span><a href=/articles/20230913a/ title="「LLM開発のためにMLOpsチームがやるべきこと」というテーマで、従来のMLOpsとの違い・ツール・構成例等について調査・整理しました。LLMとはLarge Launguage Model（大規模言語モデル）の略であり..">LLM開発のためにMLOpsチームがやるべきこと</a></li><li class="related-posts-item"><span>2023.10.11</span><span class="snscount">&#9825;6</span><a href=/articles/20231011a/ title="AzureのPrompt Flowをローカル環境で動かし、作成したフローをコードで管理する方法をご紹介します。">Prompt Flowをローカルで動かす＆コードで管理する</a></li><li class="related-posts-item"><span>2023.09.20</span><span class="snscount">&#9825;1</span><a href=/articles/20230920a/ title="フューチャーのサマーインターン2023 Summer Enginner Camp の「Goとサーバレスアーキテクチャで体験。100万台超えの大規模スマートセンサーloTプラットフォームに関わってみませんか？」というプロジェクトに参加させていただきました">Summer Enginner Camp 2023 参加記</a></li><li class="related-posts-item"><span>2023.09.19</span><span class="snscount">&#9825;28</span><a href=/articles/20230919a/ title="AzureのPrompt Flowを使ってLLMに入力するプロンプト評価の管理を行います。プロンプト評価の管理を行いたい背景として...">Prompt Flowでプロンプト評価の管理を行う</a></li><li class="related-posts-item"><span>2023.06.16</span><span class="snscount">&#9825;1</span><a href=/articles/20230616b/ title="インターン生募集の告知です">フューチャー夏のインターンシップ2023（Engineer Camp）</a></li><li class="related-posts-item"><span>2023.09.14</span><span class="snscount">&#9825;2</span><a href=/articles/20230914a/ title="LLMの実験管理ツール候補として、TruLens-Evalを検証しました。合わせて、LLMの実験管理についてまとめてみました。">【LLMOps】LLMの実験管理にTruLens-Evalを使ってみた</a></li></ul>
  </div>
            </section>
            <section class="reference-post margin-bottom-40 nav">
              
  <div class="card">
    <div id="reference" class="reference-lede"><a href="#reference" class="headerlink" title="参照されている記事"></a>この記事を参照している記事</div>
    <ul class="reference-post-link"><li class="reference-posts-item"><a href=/articles/20230914a/ title="LLMの実験管理ツール候補として、TruLens-Evalを検証しました。合わせて、LLMの実験管理についてまとめてみました。">【LLMOps】LLMの実験管理にTruLens-Evalを使ってみた</a></li><li class="reference-posts-item"><a href=/articles/20230913a/ title="「LLM開発のためにMLOpsチームがやるべきこと」というテーマで、従来のMLOpsとの違い・ツール・構成例等について調査・整理しました。LLMとはLarge Launguage Model（大規模言語モデル）の略であり..">LLM開発のためにMLOpsチームがやるべきこと</a></li></ul>
  </div>
            </section>
          </aside>
        </footer>
      </div>
    </article>
  </main>
  <aside class="col-md-3 blog-sidebar">
    <!-- START SIDEBAR  -->


<section class="toc-section">
  <h2 class="margin-top-30">目次</h2>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"><span class="toc-text">はじめに</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#LLM%E9%96%8B%E7%99%BA%E3%81%AE%E3%83%95%E3%83%AD%E3%83%BC"><span class="toc-text">LLM開発のフロー</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E7%94%A8%E6%84%8F%E3%81%99%E3%82%8B%EF%BC%88%E5%AD%A6%E7%BF%92%EF%BC%89"><span class="toc-text">1. モデルを用意する（学習）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B"><span class="toc-text">(A) ゼロからモデルを学習する</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E5%85%AC%E9%96%8B%E6%B8%88%E3%81%BF%E3%81%AE%E5%9F%BA%E7%9B%A4%E3%83%A2%E3%83%87%E3%83%AB%E3%81%8B%E3%82%89%E5%AD%A6%E7%BF%92%E3%81%99%E3%82%8B"><span class="toc-text">(B) 公開済みの基盤モデルから学習する</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-%E3%83%97%E3%83%AD%E3%83%97%E3%83%A9%E3%82%A4%E3%82%A8%E3%82%BF%E3%83%AA%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E5%88%A9%E7%94%A8%E3%81%99%E3%82%8B"><span class="toc-text">(C) プロプライエタリモデルを利用する</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E8%83%BD%E5%8A%9B%E3%82%92%E5%BC%95%E3%81%8D%E5%87%BA%E3%81%99%EF%BC%88%E6%8E%A8%E8%AB%96%EF%BC%89"><span class="toc-text">2. モデルの能力を引き出す（推論）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A0%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%83%AA%E3%83%B3%E3%82%B0"><span class="toc-text">①プロンプトエンジニアリング</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%91%A1%E3%82%B0%E3%83%A9%E3%82%A6%E3%83%B3%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%EF%BC%88%E4%BB%BB%E6%84%8F%EF%BC%89"><span class="toc-text">②グラウンディング（任意）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E3%81%BE%E3%81%A8%E3%82%81"><span class="toc-text">まとめ</span></a></li></ol>
</section>

<section class="category">
<h2 class="margin-top-30">カテゴリー</h2>
<div class="widget">
  <ul class="nav sidebar-categories margin-bottom-40">
  
  <li class=""><a href="/categories/Programming/">Programming (433)</a></li>
<li class=""><a href="/categories/Infrastructure/">Infrastructure (259)</a></li>
<li class=""><a href="/categories/Culture/">Culture (98)</a></li>
<li class=""><a href="/categories/DataScience/">DataScience (61)</a></li>
<li class=""><a href="/categories/IoT/">IoT (35)</a></li>
<li class=""><a href="/categories/DB/">DB (25)</a></li>
<li class=""><a href="/categories/DevOps/">DevOps (23)</a></li>
<li class=""><a href="/categories/%E8%AA%8D%E8%A8%BC%E8%AA%8D%E5%8F%AF/">認証認可 (21)</a></li>
<li class=""><a href="/categories/Business/">Business (21)</a></li>
<li class=""><a href="/categories/Management/">Management (17)</a></li>
<li class=""><a href="/categories/Security/">Security (15)</a></li>
<li class=""><a href="/categories/VR/">VR (13)</a></li>
<li class=""><a href="/categories/Design/">Design (11)</a></li>

  </ul>
</div>

</section>
<section class="podcast-link">
<h2 class="margin-top-30">Tech Cast</h2>

  <div class="class="widget-wrap">
  <div class="widget">
    <ul class="nav techcast">
      
    </ul>
  </div>
  </div>
  
</section>
<section class="advent-calendar">
<h2 class="margin-top-30">アドベントカレンダー</h2>
<div class="widget">
  <ul class="nav-flex">
    <li><a href="http://qiita.com/advent-calendar/2023/future" title="フューチャー Advent Calendar 2023 #Qiita" target="_blank" rel="noopener">2023年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2022/future" title="フューチャー Advent Calendar 2022 #Qiita" target="_blank" rel="noopener">2022年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2021/future" title="フューチャー Advent Calendar 2021 #Qiita" target="_blank" rel="noopener">2021年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2020/future" title="フューチャー Advent Calendar 2020 #Qiita" target="_blank" rel="noopener">2020年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2019/future" title="フューチャー Advent Calendar 2019 #Qiita" target="_blank" rel="noopener">2019年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2018/future" title="フューチャー Advent Calendar 2018 #Qiita" target="_blank" rel="noopener">2018年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2017/future" title="フューチャー Advent Calendar 2017 #Qiita" target="_blank" rel="noopener">2017年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2016/future" title="フューチャー Advent Calendar 2016 #Qiita" target="_blank" rel="noopener">2016年</a></li>
    <li><a href="http://qiita.com/advent-calendar/2015/future" title="フューチャー Advent Calendar 2015 #Qiita" target="_blank" rel="noopener">2015年</a></li>
  </ul>
</div>

</section>
<!-- END SIDEBAR -->

  </aside>
</div>

  </section>
</div>

      <!-- BEGIN PRE-FOOTER -->
    <footer>
      <div class="pre-footer">
        <div class="container">
          <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-6 col-6 pre-footer-col">
              <h2>About Us</h2>
              <p>経営とITをデザインする、フューチャーの技術ブログです。業務で利用している幅広い技術について紹介します。<br /><br /><a target="_blank" rel="noopener" href="http://www.future.co.jp/">http://www.future.co.jp/</a></p>
              <div class="social-btn twitter-btn twitter-follow-btn">
                <a href="https://twitter.com/intent/follow?screen_name=future_techblog " target="_blank" rel="nofollow noopener">
                  <i></i><span class="tw-btn-label">フューチャー技術ブログをフォロー</span>
                </a>
              </div>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-4 col-4 pre-footer-col">
              <h2>Contact</h2>
              <address class="margin-bottom-40">
                <a href="https://www.future.co.jp/recruit/recruit/rec-fresh/" title="新卒採用" target="_blank" rel="noopener">新卒採用</a><br>
                <a href="https://www.future.co.jp/recruit/recruit/rec-career/" title="キャリア採用" target="_blank" rel="noopener">キャリア採用</a><br>
                <a href="https://www.future.co.jp/contact_us/" title="お問い合わせページ" target="_blank" rel="noopener">お問い合わせ</a><br>
                <a href="https://www.future.co.jp/architect/socialmediapolicy/" title="ソーシャルメディアポリシー" target="_blank" rel="noopener">メディアポリシー</a><br><br>
                <a href="mailto:techblog@future.co.jp">techblog@future.co.jp</a>
              </address>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6 col-6 pre-footer-col">
              <h2>Contents</h2>
              <a href="https://future-architect.github.io/coding-standards/" title="Future Enterprise Coding Standards" target="_blank" rel="noopener">コーディング規約</a><br>
              <a href="https://future-architect.github.io/typescript-guide/" title="仕事ですぐに使えるTypeScript" target="_blank" rel="noopener">仕事ですぐに使えるTypeScript</a><br>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-3 col-3 pre-footer-col">
              <h2>Event</h2>
              <a href="https://future.connpass.com/" title="経営とITをデザインするフューチャーの勉強会です" target="_blank" rel="noopener">connpass</a><br>
              <a href="https://www.future.co.jp/futureinsightseminar/" title="フューチャーインサイトセミナー" target="_blank" rel="noopener">Webセミナー</a><br>
            </div>
            <div class="col-lg-2 col-md-4 col-sm-3 col-3 pre-footer-col">
              <h2>SNS</h2>
              <a href="https://github.com/future-architect" title="Future's official open source repositories" target="_blank" rel="noopener">GitHub</a><br>
              <a href="https://qiita.com/organizations/future" title="フューチャーのQiita Organizationです" target="_blank" rel="noopener">Qiita</a><br>
              <a href="https://note.future.co.jp/" title="フューチャーの公式note" target="_blank" rel="noopener">未来報</a><br>
              <a href="https://www.youtube.com/channel/UCJUSwYYd0CkGgmEKAW7QVpw" title="フューチャーYoutubeチャネル" target="_blank" rel="noopener">Youtube</a>
            </div>
          </div>
        </div>
      </div>
      <div class="footer">
        <div class="container">
          <div class="row">
            <div class="col-md-6 col-sm-6 padding-top-10">
              &copy; 2024 フューチャー技術ブログ<br>
            </div>
          </div>
        </div>
      </div>
    </footer>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-X1C28R8H0M"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-X1C28R8H0M');
  gtag('config', 'UA-74047147-1'); // 過渡期対応
</script>

  </div>
</body>
</html>
